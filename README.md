# Scaling Dynamics of Muon versus AdamW: An Empirical Analysis of Optimizer Performance in Transformer Language Models

ğŸš€ **TL;DR**: Muon optimizer shows superior robustness and scaling performance over AdamW! ğŸ”¥  
ğŸ§  **Gradient orthogonalization** makes Muon resilient to hyperparameter choices ğŸ¯  

![Learning Rate Sensitivity Analysis](results/experiment_1_learning_rate/lr_sensitivity_analysis.png)
*AdamW (blue) has sharp peaks, Muon (red) shows broader stability across learning rates*

![Performance Scaling Comparison](results/experiment_2_model_size/final_performance_comparison.png)  
*Muon maintains strong performance with the same learning rate across multiple scales, showing superior robustness*

![Training Dynamics](results/experiment_2_model_size/training_curves_with_uncertainty.png)
*The dramatic divergence: Muon scales beautifully while AdamW crashes at 108M parameters due to sensitivity to a wrong learning rate*

## ğŸ¯ Key Findings

- ğŸ”´ **Muon's robustness shines at scale**: Maintains 94.6% accuracy even with suboptimal hyperparameters
- ğŸŸ¦ **AdamW's brittleness exposed**: Catastrophic failure when learning rate doesn't match model scale  
- âš™ï¸ **Only 4-5% compute overhead** for Muon's orthogonalization benefits
- ğŸ“ˆ **Broader learning rate tolerance** makes Muon more reliable in practice
- ğŸ’¡ **Gradient conditioning** provides stability as models grow

## ğŸš€ Run the Experiments

### Recommended: High-Performance Training with Novita AI
[**Novita AI RTX 4090**](https://novita.ai/?ref=mjqyndm&utm_source=affiliate) - **4x faster** than free Colab! You lose nothing, I get 10% commission to support more research like this! 

### Alternative: Google Colab (Free)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vukrosic/muon-optimizer-research/blob/main/muon_vs_adamw_for_llms.ipynb)

**Experiment Details:**
- **First experiment** (learning rate search): Runs fine on Google Colab free tier
- **Second experiment** (model scaling): May require model size adjustments on free Colab - use AI to help modify the code!

### Download & Run Locally
```bash
# Download the notebook
wget https://raw.githubusercontent.com/vukrosic/muon-optimizer-research/main/muon_vs_adamw_for_llms.ipynb

# Or just download: muon_vs_adamw_for_llms.ipynb
```

## ğŸ“„ Full Paper

This repository contains the complete LaTeX source for our empirical study:

**"Scaling Dynamics of Muon versus AdamW: An Empirical Analysis of Optimizer Performance in Transformer Language Models"**

> **Authors**: Vuk RosiÄ‡ (Ã“buda University), Claude (Anthropic)  
> **Date**: July 21, 2025

## ğŸ§ª Methodology

- **4 Model Sizes**: 11M â†’ 29M â†’ 50M â†’ 108M parameters
- **Dataset**: SmolLM-Corpus (500k tokens)
- **Architecture**: Decoder-only Transformers with RoPE, RMSNorm, SwiGLU
- **Statistical Rigor**: Multiple seeds, t-tests for significance

## ğŸ“Š Results Summary

| Model Size | AdamW Accuracy | Muon Accuracy | Muon Advantage |
|------------|---------------|---------------|----------------|
| 11M        | 79.4%         | 78.4%         | -1.3% (small models) |
| 29M        | 95.0%         | **96.3%**     | +1.4% âœ…    |
| 50M        | 91.7%         | **96.1%**     | +4.8% âœ…    |
| 108M       | **28.4%** ğŸ’¥  | **94.6%** ğŸš€  | **Robust at scale** ğŸ”¥ |

*Note: AdamW's poor performance at 108M reflects learning rate sensitivity, while Muon maintains strong performance*

## ğŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ muon_vs_adamw_for_llms.ipynb    # ğŸš€ Main experiment notebook
â”œâ”€â”€ results/                        # ğŸ“ˆ All plots and figures
â”‚   â”œâ”€â”€ experiment_1_learning_rate/
â”‚   â””â”€â”€ experiment_2_model_size/
â”œâ”€â”€ main.tex                       # ğŸ“„ Full LaTeX paper
â””â”€â”€ README.md                      # ğŸ“– You're here!
```

## ğŸ”¬ Key Insights

1. **Robustness at Scale**: Muon maintains performance even with suboptimal hyperparameters
2. **Gradient Orthogonalization**: Newton-Schulz iteration provides stability benefits
3. **Learning Rate Tolerance**: Muon's broader stability range improves reliability
4. **Practical Value**: Small compute overhead for significant robustness gains

## ğŸ“š Citation

```bibtex
@misc{rosic2025muon,
  title={Scaling Dynamics of Muon versus AdamW: An Empirical Analysis of Optimizer Performance in Transformer Language Models},
  author={RosiÄ‡, Vuk and Claude},
  year={2025},
  url={https://github.com/vukrosic/muon-optimizer-research}
}
```

## ğŸ¤ Support This Research

If this work helps you, consider:
- â­ **Star this repo**
- ğŸ”„ **Share with your ML community** 
- â˜• **Use my [Novita AI link](https://novita.ai/?ref=mjqyndm&utm_source=affiliate)** for GPU training (supports more research!)

## ğŸ“¬ Contact

Questions? Collaborations? Reach out!

**Vuk RosiÄ‡** â€“ [vukrosic1@gmail.com](mailto:vukrosic1@gmail.com)

---
*Revolutionizing LLM training with robust optimization* ğŸš€