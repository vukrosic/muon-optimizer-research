{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fce7zdNHk7Z"
      },
      "source": [
        "You must restart kernel after you install the cell below. Go to Kernel -> Restart Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHcV7_ehx825",
        "outputId": "dbc16312-18be-4315-f54c-6ba762e3b3bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets)\n",
            "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tqdm>=4.66.3 (from datasets)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.3.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.24.0->datasets)\n",
            "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, tzdata, tqdm, requests, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 hf-xet-1.1.5 huggingface-hub-0.33.4 multidict-6.6.3 multiprocess-0.70.16 pandas-2.3.1 propcache-0.3.2 pyarrow-21.0.0 requests-2.32.4 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d70Gow9BHk7e"
      },
      "source": [
        "Now that you installed the cell above, restart the kernel. Go to Kernel -> Restart Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCBDSw8PHk7e",
        "outputId": "76b88f02-31b7-47f3-f166-f39dc8f442c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.10.3)\n",
            "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.53.2)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.3.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m839.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hInstalling collected packages: seaborn\n",
            "Successfully installed seaborn-0.13.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib transformers seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lPJ4rTCKO4U"
      },
      "source": [
        "## Learning rate search for Muon and AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-pv_ddrHk7g",
        "outputId": "6cfe22e2-3a86-411a-f33b-677871ca09d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
            "Collecting torch==2.7.0\n",
            "  Downloading https://download.pytorch.org/whl/cu128/torch-2.7.0%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.22.1)\n",
            "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.7.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (2024.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.61 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cuda_nvrtc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.57 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cuda_runtime_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.57 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cuda_cupti_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.7.1.26 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cudnn_cu12-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.3.14 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cublas_cu12-12.8.3.14-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.41 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cufft_cu12-11.3.3.41-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.55 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_curand_cu12-10.3.9.55-py3-none-manylinux_2_27_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.2.55 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cusolver_cu12-11.7.2.55-py3-none-manylinux_2_27_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.7.53 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cusparse_cu12-12.5.7.53-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.10/site-packages (from torch==2.7.0) (2.26.2)\n",
            "Collecting nvidia-nvtx-cu12==12.8.55 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_nvtx_cu12-12.8.55-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.61 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.0.11 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/nvidia_cufile_cu12-1.13.0.11-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.10/site-packages (from triton==3.3.0->torch==2.7.0) (68.2.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu128/torchvision-0.22.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/torchvision-0.22.0%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu128/torchaudio-2.7.1%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu128/torchaudio-2.7.0%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.7.0) (2.1.3)\n",
            "Downloading https://download.pytorch.org/whl/cu128/torch-2.7.0%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl (1097.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 GB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cublas_cu12-12.8.3.14-py3-none-manylinux_2_27_x86_64.whl (609.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m609.6/609.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cuda_cupti_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cuda_nvrtc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cuda_runtime_cu12-12.8.57-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cudnn_cu12-9.7.1.26-py3-none-manylinux_2_27_x86_64.whl (726.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m726.9/726.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cufft_cu12-11.3.3.41-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cufile_cu12-1.13.0.11-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_curand_cu12-10.3.9.55-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cusolver_cu12-11.7.2.55-py3-none-manylinux_2_27_x86_64.whl (260.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.4/260.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_cusparse_cu12-12.5.7.53-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (292.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_nvjitlink_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.2/39.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/nvidia_nvtx_cu12-12.8.55-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/torchvision-0.22.0%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu128/torchaudio-2.7.0%2Bcu128-cp310-cp310-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.3.1\n",
            "    Uninstalling triton-3.3.1:\n",
            "      Successfully uninstalled triton-3.3.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.7.1\n",
            "    Uninstalling torch-2.7.1:\n",
            "      Successfully uninstalled torch-2.7.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.22.1\n",
            "    Uninstalling torchvision-0.22.1:\n",
            "      Successfully uninstalled torchvision-0.22.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.7.1\n",
            "    Uninstalling torchaudio-2.7.1:\n",
            "      Successfully uninstalled torchaudio-2.7.1\n",
            "Successfully installed nvidia-cublas-cu12-12.8.3.14 nvidia-cuda-cupti-cu12-12.8.57 nvidia-cuda-nvrtc-cu12-12.8.61 nvidia-cuda-runtime-cu12-12.8.57 nvidia-cudnn-cu12-9.7.1.26 nvidia-cufft-cu12-11.3.3.41 nvidia-cufile-cu12-1.13.0.11 nvidia-curand-cu12-10.3.9.55 nvidia-cusolver-cu12-11.7.2.55 nvidia-cusparse-cu12-12.5.7.53 nvidia-nvjitlink-cu12-12.8.61 nvidia-nvtx-cu12-12.8.55 torch-2.7.0+cu128 torchaudio-2.7.0+cu128 torchvision-0.22.0+cu128 triton-3.3.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EApHJd0rHk7g"
      },
      "source": [
        "You will possibly need to reset the kernel again after installing torch, make sure the cell above says 2.7.0+cu128, otherwise reset the kernel again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOKJpnyUHk7h",
        "outputId": "1bb12697-67d7-4f77-a6f6-504554a59e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "# restart kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjkMdFvHk7i"
      },
      "source": [
        "Cell below was run on T4, which is the free Google Colab GPU, so it's small enough for that, if you are running it on a stronger GPU, you can ask Claude Sonnet to increase size of the ablations or models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "069a3f9bce3a49f880b43f56afcae4b0",
            "de9398edea76440395e08e3a97a6cedb",
            "12eb27f6dd294c048e41aeaa18c531fd",
            "29712a917e2d4276ae2e3929569f93b1",
            "45998e5f631c4459846d379c3f9ecad8",
            "8813d443da7b48ec918048f7647227a6",
            "e14c5ec6957d49c0a8faf905ffc1111f",
            "c0b0ab8d523a4a1c9dfae056f7d2aa31",
            "41202f2657204911a36c29b7203dff80",
            "61ddf7020bb947a29f42690bb43a6417",
            "4bf129bde6224e8285a8a2e9f5c0bb42",
            "a6ad6929431e4e5d8727ad64b3a58ddc",
            "07436c51fd514990acd73d17dff41e9e",
            "76bb84c5877945cb96eca45add755082",
            "fe9818de3cb948cc815fc956b401d7f1",
            "072b91c47fe84dbfbdb454641e1c0c6a",
            "110f0a4c3e0244f4a3a3529f8242b36a",
            "de02728f910c4d6c92bc08ed2f46aa17",
            "ba748793292b4a5dbdebe0ebb8e60871",
            "87ff82da16ad4d8e86350ec6aa0a4e40",
            "7dec43e84657447ca77f75072f8ebb1c",
            "31de8f9df7254e67a79d6975fa7f3778"
          ]
        },
        "id": "WESkKeJmKUBD",
        "outputId": "028c5787-72c7-4eff-d782-6a6fe38edfc9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Device: CUDA\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n",
            "🌱 Set all seeds to 42\n",
            "🔍 COMPREHENSIVE LEARNING RATE SEARCH: MUON vs ADAMW\n",
            "================================================================================\n",
            "🏗️ Model Architecture: 256d, 4L, 8H\n",
            "📊 AdamW LRs: [1e-05, 3e-05, 0.0001, 0.0003, 0.001, 0.003, 0.01]\n",
            "📊 Muon LRs: [0.001, 0.003, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05]\n",
            "⏱️ Training steps: 600\n",
            "📦 Batch size: 32\n",
            "Loading dataset...\n"
          ]
        },
        {
          "data": {
            "application/application/vnd.jupyter.widget-state+json": {
              "model_id": "069a3f9bce3a49f880b43f56afcae4b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6ad6929431e4e5d8727ad64b3a58ddc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 200 documents\n",
            "Tokenizing texts...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenizing:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "Tokenizing:  27%|██▋       | 54/200 [00:00<00:00, 537.14it/s]\u001b[A\n",
            "Tokenizing:  62%|██████▏   | 123/200 [00:00<00:00, 624.66it/s]\u001b[A\n",
            "Tokenizing: 100%|██████████| 200/200 [00:00<00:00, 633.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 30,000 tokens\n",
            "\n",
            "==================================================\n",
            "🔵 TESTING ADAMW\n",
            "==================================================\n",
            "🚀 ADAMW LR=1e-05\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 1e-05: Final Loss=8.459477424621582, Val Acc=0.070\n",
            "🚀 ADAMW LR=3e-05\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 3e-05: Final Loss=6.6022467613220215, Val Acc=0.183\n",
            "🚀 ADAMW LR=0.0001\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.0001: Final Loss=4.173043251037598, Val Acc=0.371\n",
            "🚀 ADAMW LR=0.0003\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.0003: Final Loss=1.478623628616333, Val Acc=0.777\n",
            "🚀 ADAMW LR=0.001\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.001: Final Loss=0.16272294521331787, Val Acc=0.968\n",
            "🚀 ADAMW LR=0.003\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.003: Final Loss=0.12793155014514923, Val Acc=0.972\n",
            "🚀 ADAMW LR=0.01\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.01: Final Loss=3.3094286918640137, Val Acc=0.284\n",
            "\n",
            "==================================================\n",
            "🔴 TESTING MUON\n",
            "==================================================\n",
            "🚀 MUON LR=0.001\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.001: Final Loss=3.733657121658325, Val Acc=0.430\n",
            "🚀 MUON LR=0.003\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.003: Final Loss=0.5717266798019409, Val Acc=0.953\n",
            "🚀 MUON LR=0.005\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.005: Final Loss=0.13839775323867798, Val Acc=0.983\n",
            "🚀 MUON LR=0.01\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.01: Final Loss=0.08118744194507599, Val Acc=0.984\n",
            "🚀 MUON LR=0.015\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.015: Final Loss=0.08615727722644806, Val Acc=0.983\n",
            "🚀 MUON LR=0.02\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.02: Final Loss=0.09520182758569717, Val Acc=0.980\n",
            "🚀 MUON LR=0.03\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.03: Final Loss=0.09264008700847626, Val Acc=0.980\n",
            "🚀 MUON LR=0.05\n",
            "🌱 Set all seeds to 42\n",
            "  ✅ LR 0.05: Final Loss=0.1162719801068306, Val Acc=0.974\n",
            "\n",
            "💾 Saving results to results/lr_search_20250721_182518\n",
            "✅ All results saved to results/lr_search_20250721_182518\n",
            "\n",
            "🎉 LEARNING RATE SEARCH COMPLETED IN 20.1 MINUTES!\n",
            "✅ Results saved to 'results/' folder\n",
            "📊 Check the generated plots and report for optimal learning rates\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "import subprocess\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages for Colab\"\"\"\n",
        "    packages = ['datasets', 'transformers', 'accelerate', 'scipy', 'seaborn']\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "install_packages()\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"🌱 Set all seeds to {seed}\")\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    # Model architecture (FIXED)\n",
        "    d_model: int = 256\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 4\n",
        "    d_ff: int = 1024\n",
        "    max_seq_len: int = 256\n",
        "\n",
        "    # Training (FIXED)\n",
        "    batch_size: int = 32  # Increased for faster training\n",
        "    max_steps: int = 600  # Short but enough to see trends\n",
        "    eval_every: int = 100\n",
        "\n",
        "    # Data (FIXED)\n",
        "    num_documents: int = 200\n",
        "    max_tokens: int = 30000\n",
        "    vocab_size: Optional[int] = None\n",
        "\n",
        "    # Optimization (VARIABLE - this is what we're testing)\n",
        "    learning_rate: float = 1e-3  # This will be varied\n",
        "    weight_decay: float = 0.01   # Fixed low value\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # System (FIXED)\n",
        "    use_amp: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "@torch.compile\n",
        "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
        "    \"\"\"Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\"\"\"\n",
        "    assert G.ndim >= 2\n",
        "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
        "    X = G.bfloat16()\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.mT\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    return X\n",
        "\n",
        "class Muon(torch.optim.Optimizer):\n",
        "    \"\"\"Muon - MomentUm Orthogonalized by Newton-schulz\"\"\"\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
        "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                g = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                if \"momentum_buffer\" not in state:\n",
        "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
        "\n",
        "                buf = state[\"momentum_buffer\"]\n",
        "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
        "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
        "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
        "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n",
        "\n",
        "def load_data(config: ExperimentConfig):\n",
        "    \"\"\"Load and tokenize data\"\"\"\n",
        "    print(f\"Loading dataset...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", token=False)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True, token=False)\n",
        "\n",
        "    texts = []\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= config.num_documents:\n",
        "            break\n",
        "        texts.append(item[\"text\"][:1500])  # Shorter texts for faster processing\n",
        "\n",
        "    print(f\"Loaded {len(texts)} documents\")\n",
        "    config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    return texts, tokenizer\n",
        "\n",
        "class TextTokenDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], tokenizer, seq_len: int = 256, max_tokens: int = 30000):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        print(\"Tokenizing texts...\")\n",
        "        all_tokens = []\n",
        "        for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "            tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "            all_tokens.extend(tokens)\n",
        "\n",
        "        self.tokens = all_tokens[:max_tokens]\n",
        "        print(f\"Using {len(self.tokens):,} tokens\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "class Rotary(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        angular_freq = (1 / 10000) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
        "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
        "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
        "        self.register_buffer('cos', theta.cos(), persistent=False)\n",
        "        self.register_buffer('sin', theta.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x_BTHD: torch.Tensor):\n",
        "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
        "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
        "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
        "        y1 = x1 * cos + x2 * sin\n",
        "        y2 = x1 * (-sin) + x2 * cos\n",
        "        return torch.cat((y1, y2), 3).type_as(x_BTHD)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.rotary = Rotary(self.d_k, max_seq_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        Q = self.rotary(Q)\n",
        "        K = self.rotary(K)\n",
        "\n",
        "        attn_output = F.scaled_dot_product_attention(Q, K, V, is_causal=True, dropout_p=0.0)\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
        "        return self.w_o(attn_output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.silu(self.linear1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, max_seq_len)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention(self.norm1(x))\n",
        "        x = x + attn_out\n",
        "        ff_out = self.feed_forward(self.norm2(x))\n",
        "        x = x + ff_out\n",
        "        return x\n",
        "\n",
        "class MinimalLLM(nn.Module):\n",
        "    def __init__(self, config: ExperimentConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.max_seq_len)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "        self.norm = nn.RMSNorm(config.d_model)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = F.linear(x, self.token_embedding.weight)\n",
        "        return logits\n",
        "\n",
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.metrics = {}\n",
        "\n",
        "    def log_step(self, step: int, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            if key not in self.metrics:\n",
        "                self.metrics[key] = []\n",
        "            self.metrics[key].append((step, value))\n",
        "\n",
        "def setup_optimizer(model: nn.Module, optimizer_type: str, learning_rate: float, config: ExperimentConfig):\n",
        "    \"\"\"Setup optimizer with specific learning rate\"\"\"\n",
        "\n",
        "    if optimizer_type == 'muon':\n",
        "        muon_params = []\n",
        "        adamw_params = []\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if (param.ndim == 2 and\n",
        "                'token_embedding' not in name and\n",
        "                'norm' not in name and\n",
        "                param.requires_grad):\n",
        "                muon_params.append(param)\n",
        "            else:\n",
        "                adamw_params.append(param)\n",
        "\n",
        "        muon_optimizer = Muon(muon_params, lr=learning_rate, momentum=0.95)\n",
        "        adamw_optimizer = torch.optim.AdamW(adamw_params, lr=learning_rate*0.1, weight_decay=config.weight_decay)\n",
        "\n",
        "        return [muon_optimizer, adamw_optimizer]\n",
        "\n",
        "    else:  # adamw\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=config.weight_decay)\n",
        "        return [optimizer]\n",
        "\n",
        "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ExperimentConfig) -> Dict:\n",
        "    \"\"\"Quick model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            if i >= 3:  # Very quick eval\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            with autocast('cuda', enabled=config.use_amp):\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return {\n",
        "        'val_loss': avg_loss,\n",
        "        'val_accuracy': accuracy,\n",
        "        'val_perplexity': perplexity\n",
        "    }\n",
        "\n",
        "def train_with_learning_rate(optimizer_type: str, learning_rate: float, config: ExperimentConfig,\n",
        "                           train_loader: DataLoader, val_loader: DataLoader) -> Dict:\n",
        "    \"\"\"Train model with specific learning rate\"\"\"\n",
        "\n",
        "    print(f\"🚀 {optimizer_type.upper()} LR={learning_rate}\")\n",
        "\n",
        "    # Initialize model\n",
        "    set_seed(42)  # Same initialization for all runs\n",
        "    model = MinimalLLM(config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Setup optimizer with this learning rate\n",
        "    optimizers = setup_optimizer(model, optimizer_type, learning_rate, config)\n",
        "\n",
        "    # Setup schedulers (no decay for clean LR comparison)\n",
        "    # schedulers = []\n",
        "    # for optimizer in optimizers:\n",
        "    #     scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0)\n",
        "    #     schedulers.append(scheduler)\n",
        "\n",
        "    scaler = GradScaler('cuda') if config.use_amp else None\n",
        "    tracker = MetricsTracker()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    step = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Track if training becomes unstable\n",
        "    min_loss = float('inf')\n",
        "    unstable_count = 0\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            for optimizer in optimizers:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                if config.use_amp:\n",
        "                    with autocast('cuda'):\n",
        "                        logits = model(x)\n",
        "                        loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        print(f\"  💥 NaN/Inf loss at step {step}\")\n",
        "                        return {'failed': True, 'reason': 'nan_loss', 'step': step}\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "\n",
        "                    # Unscale and clip gradients\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    # Step optimizers\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.step(optimizer)\n",
        "\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    logits = model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        print(f\"  💥 NaN/Inf loss at step {step}\")\n",
        "                        return {'failed': True, 'reason': 'nan_loss', 'step': step}\n",
        "\n",
        "                    loss.backward()\n",
        "\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"  💥 Runtime error at step {step}: {e}\")\n",
        "                return {'failed': True, 'reason': 'runtime_error', 'step': step}\n",
        "\n",
        "            # Check for training instability\n",
        "            if loss.item() > min_loss * 2 and step > 100:\n",
        "                unstable_count += 1\n",
        "                if unstable_count > 10:\n",
        "                    print(f\"  ⚠️ Training became unstable at step {step}\")\n",
        "                    return {'failed': True, 'reason': 'unstable', 'step': step}\n",
        "            else:\n",
        "                min_loss = min(min_loss, loss.item())\n",
        "                unstable_count = 0\n",
        "\n",
        "            # Log metrics\n",
        "            if step % 50 == 0:\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    accuracy = (predictions == y).float().mean().item()\n",
        "                    perplexity = math.exp(min(loss.item(), 20))\n",
        "\n",
        "                tracker.log_step(\n",
        "                    step,\n",
        "                    train_loss=loss.item(),\n",
        "                    train_accuracy=accuracy,\n",
        "                    train_perplexity=perplexity,\n",
        "                    grad_norm=grad_norm.item(),\n",
        "                    learning_rate=optimizers[0].param_groups[0]['lr']\n",
        "                )\n",
        "\n",
        "            # Evaluation\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = evaluate_model(model, val_loader, config)\n",
        "                for key, value in eval_metrics.items():\n",
        "                    tracker.log_step(step, **{key: value})\n",
        "\n",
        "            step += 1\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Final evaluation\n",
        "    final_eval = evaluate_model(model, val_loader, config)\n",
        "\n",
        "    # Get final training loss\n",
        "    final_train_loss = None\n",
        "    if 'train_loss' in tracker.metrics and len(tracker.metrics['train_loss']) > 0:\n",
        "        final_train_loss = tracker.metrics['train_loss'][-1][1]\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'failed': False,\n",
        "        'tracker': tracker,\n",
        "        'training_time': training_time,\n",
        "        'final_metrics': final_eval,\n",
        "        'final_train_loss': final_train_loss,\n",
        "        'learning_rate': learning_rate\n",
        "    }\n",
        "\n",
        "def run_learning_rate_search():\n",
        "    \"\"\"Run comprehensive learning rate search\"\"\"\n",
        "\n",
        "    print(\"🔍 COMPREHENSIVE LEARNING RATE SEARCH: MUON vs ADAMW\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Fixed configuration\n",
        "    config = ExperimentConfig()\n",
        "\n",
        "    # Learning rate ranges to test\n",
        "    adamw_lrs = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
        "    muon_lrs = [0.001, 0.003, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05]\n",
        "\n",
        "    print(f\"🏗️ Model Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H\")\n",
        "    print(f\"📊 AdamW LRs: {adamw_lrs}\")\n",
        "    print(f\"📊 Muon LRs: {muon_lrs}\")\n",
        "    print(f\"⏱️ Training steps: {config.max_steps}\")\n",
        "    print(f\"📦 Batch size: {config.batch_size}\")\n",
        "\n",
        "    # Load data once\n",
        "    texts, tokenizer = load_data(config)\n",
        "    dataset = TextTokenDataset(texts, tokenizer, config.max_seq_len, config.max_tokens)\n",
        "\n",
        "    # Split data\n",
        "    val_size = len(dataset) // 10\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "    # Run experiments\n",
        "    all_results = {\n",
        "        'adamw': {},\n",
        "        'muon': {},\n",
        "        'config': config\n",
        "    }\n",
        "\n",
        "    # Test AdamW\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"🔵 TESTING ADAMW\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for lr in adamw_lrs:\n",
        "        result = train_with_learning_rate('adamw', lr, config, train_loader, val_loader)\n",
        "        all_results['adamw'][lr] = result\n",
        "\n",
        "        if result['failed']:\n",
        "            print(f\"  ❌ LR {lr}: FAILED ({result['reason']})\")\n",
        "        else:\n",
        "            final_loss = result['final_train_loss'] if result['final_train_loss'] else \"N/A\"\n",
        "            val_acc = result['final_metrics']['val_accuracy']\n",
        "            print(f\"  ✅ LR {lr}: Final Loss={final_loss}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "    # Test Muon\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"🔴 TESTING MUON\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    for lr in muon_lrs:\n",
        "        result = train_with_learning_rate('muon', lr, config, train_loader, val_loader)\n",
        "        all_results['muon'][lr] = result\n",
        "\n",
        "        if result['failed']:\n",
        "            print(f\"  ❌ LR {lr}: FAILED ({result['reason']})\")\n",
        "        else:\n",
        "            final_loss = result['final_train_loss'] if result['final_train_loss'] else \"N/A\"\n",
        "            val_acc = result['final_metrics']['val_accuracy']\n",
        "            print(f\"  ✅ LR {lr}: Final Loss={final_loss}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "    # Save and analyze results\n",
        "    save_lr_search_results(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def save_lr_search_results(all_results: Dict):\n",
        "    \"\"\"Save learning rate search results\"\"\"\n",
        "\n",
        "    # Create results directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_dir = f\"results/lr_search_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n💾 Saving results to {results_dir}\")\n",
        "\n",
        "    # Generate plots\n",
        "    generate_lr_plots(all_results, results_dir)\n",
        "\n",
        "    # Generate report\n",
        "    generate_lr_report(all_results, results_dir)\n",
        "\n",
        "    # Save raw data\n",
        "    save_lr_raw_data(all_results, results_dir)\n",
        "\n",
        "    print(f\"✅ All results saved to {results_dir}\")\n",
        "\n",
        "def generate_lr_plots(all_results: Dict, results_dir: str):\n",
        "    \"\"\"Generate learning rate analysis plots\"\"\"\n",
        "\n",
        "    # Set style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    # 1. Learning Rate vs Final Metrics\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Learning Rate Sensitivity Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Collect data for successful runs\n",
        "    adamw_data = {'lrs': [], 'final_loss': [], 'val_loss': [], 'val_acc': [], 'val_ppl': []}\n",
        "    muon_data = {'lrs': [], 'final_loss': [], 'val_loss': [], 'val_acc': [], 'val_ppl': []}\n",
        "\n",
        "    for lr, result in all_results['adamw'].items():\n",
        "        if not result['failed'] and result['final_train_loss'] is not None:\n",
        "            adamw_data['lrs'].append(lr)\n",
        "            adamw_data['final_loss'].append(result['final_train_loss'])\n",
        "            adamw_data['val_loss'].append(result['final_metrics']['val_loss'])\n",
        "            adamw_data['val_acc'].append(result['final_metrics']['val_accuracy'])\n",
        "            adamw_data['val_ppl'].append(result['final_metrics']['val_perplexity'])\n",
        "\n",
        "    for lr, result in all_results['muon'].items():\n",
        "        if not result['failed'] and result['final_train_loss'] is not None:\n",
        "            muon_data['lrs'].append(lr)\n",
        "            muon_data['final_loss'].append(result['final_train_loss'])\n",
        "            muon_data['val_loss'].append(result['final_metrics']['val_loss'])\n",
        "            muon_data['val_acc'].append(result['final_metrics']['val_accuracy'])\n",
        "            muon_data['val_ppl'].append(result['final_metrics']['val_perplexity'])\n",
        "\n",
        "    # Final Training Loss\n",
        "    axes[0,0].semilogx(adamw_data['lrs'], adamw_data['final_loss'], 'o-', label='AdamW', color='blue', linewidth=2, markersize=8)\n",
        "    axes[0,0].semilogx(muon_data['lrs'], muon_data['final_loss'], 's-', label='Muon', color='red', linewidth=2, markersize=8)\n",
        "    axes[0,0].set_title('Final Training Loss vs Learning Rate')\n",
        "    axes[0,0].set_xlabel('Learning Rate')\n",
        "    axes[0,0].set_ylabel('Final Training Loss')\n",
        "    axes[0,0].set_yscale('log')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Validation Loss\n",
        "    axes[0,1].semilogx(adamw_data['lrs'], adamw_data['val_loss'], 'o-', label='AdamW', color='blue', linewidth=2, markersize=8)\n",
        "    axes[0,1].semilogx(muon_data['lrs'], muon_data['val_loss'], 's-', label='Muon', color='red', linewidth=2, markersize=8)\n",
        "    axes[0,1].set_title('Validation Loss vs Learning Rate')\n",
        "    axes[0,1].set_xlabel('Learning Rate')\n",
        "    axes[0,1].set_ylabel('Validation Loss')\n",
        "    axes[0,1].set_yscale('log')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Validation Accuracy\n",
        "    axes[1,0].semilogx(adamw_data['lrs'], adamw_data['val_acc'], 'o-', label='AdamW', color='blue', linewidth=2, markersize=8)\n",
        "    axes[1,0].semilogx(muon_data['lrs'], muon_data['val_acc'], 's-', label='Muon', color='red', linewidth=2, markersize=8)\n",
        "    axes[1,0].set_title('Validation Accuracy vs Learning Rate')\n",
        "    axes[1,0].set_xlabel('Learning Rate')\n",
        "    axes[1,0].set_ylabel('Validation Accuracy')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Validation Perplexity\n",
        "    axes[1,1].semilogx(adamw_data['lrs'], adamw_data['val_ppl'], 'o-', label='AdamW', color='blue', linewidth=2, markersize=8)\n",
        "    axes[1,1].semilogx(muon_data['lrs'], muon_data['val_ppl'], 's-', label='Muon', color='red', linewidth=2, markersize=8)\n",
        "    axes[1,1].set_title('Validation Perplexity vs Learning Rate')\n",
        "    axes[1,1].set_xlabel('Learning Rate')\n",
        "    axes[1,1].set_ylabel('Validation Perplexity')\n",
        "    axes[1,1].set_yscale('log')\n",
        "    axes[1,1].legend()\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{results_dir}/lr_sensitivity_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Training curves for best learning rates\n",
        "    best_adamw_lr = adamw_data['lrs'][np.argmax(adamw_data['val_acc'])] if adamw_data['val_acc'] else None\n",
        "    best_muon_lr = muon_data['lrs'][np.argmax(muon_data['val_acc'])] if muon_data['val_acc'] else None\n",
        "\n",
        "    if best_adamw_lr and best_muon_lr:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        fig.suptitle(f'Training Curves: Best LRs (AdamW: {best_adamw_lr}, Muon: {best_muon_lr})', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Training loss\n",
        "        adamw_result = all_results['adamw'][best_adamw_lr]\n",
        "        muon_result = all_results['muon'][best_muon_lr]\n",
        "\n",
        "        if 'train_loss' in adamw_result['tracker'].metrics:\n",
        "            steps, losses = zip(*adamw_result['tracker'].metrics['train_loss'])\n",
        "            axes[0].plot(steps, losses, label='AdamW', color='blue', linewidth=2)\n",
        "\n",
        "        if 'train_loss' in muon_result['tracker'].metrics:\n",
        "            steps, losses = zip(*muon_result['tracker'].metrics['train_loss'])\n",
        "            axes[0].plot(steps, losses, label='Muon', color='red', linewidth=2)\n",
        "\n",
        "        axes[0].set_title('Training Loss')\n",
        "        axes[0].set_xlabel('Steps')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_yscale('log')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Validation loss\n",
        "        if 'val_loss' in adamw_result['tracker'].metrics:\n",
        "            steps, losses = zip(*adamw_result['tracker'].metrics['val_loss'])\n",
        "            axes[1].plot(steps, losses, label='AdamW', color='blue', linewidth=2)\n",
        "\n",
        "        if 'val_loss' in muon_result['tracker'].metrics:\n",
        "            steps, losses = zip(*muon_result['tracker'].metrics['val_loss'])\n",
        "            axes[1].plot(steps, losses, label='Muon', color='red', linewidth=2)\n",
        "\n",
        "        axes[1].set_title('Validation Loss')\n",
        "        axes[1].set_xlabel('Steps')\n",
        "        axes[1].set_ylabel('Loss')\n",
        "        axes[1].set_yscale('log')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Validation accuracy\n",
        "        if 'val_accuracy' in adamw_result['tracker'].metrics:\n",
        "            steps, accs = zip(*adamw_result['tracker'].metrics['val_accuracy'])\n",
        "            axes[2].plot(steps, accs, label='AdamW', color='blue', linewidth=2)\n",
        "\n",
        "        if 'val_accuracy' in muon_result['tracker'].metrics:\n",
        "            steps, accs = zip(*muon_result['tracker'].metrics['val_accuracy'])\n",
        "            axes[2].plot(steps, accs, label='Muon', color='red', linewidth=2)\n",
        "\n",
        "        axes[2].set_title('Validation Accuracy')\n",
        "        axes[2].set_xlabel('Steps')\n",
        "        axes[2].set_ylabel('Accuracy')\n",
        "        axes[2].legend()\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{results_dir}/best_lr_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # 3. Success/Failure visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    fig.suptitle('Learning Rate Stability Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # AdamW stability\n",
        "    adamw_lrs_all = list(all_results['adamw'].keys())\n",
        "    adamw_success = [1 if not all_results['adamw'][lr]['failed'] else 0 for lr in adamw_lrs_all]\n",
        "\n",
        "    axes[0].semilogx(adamw_lrs_all, adamw_success, 'o-', color='blue', linewidth=2, markersize=8)\n",
        "    axes[0].set_title('AdamW Training Stability')\n",
        "    axes[0].set_xlabel('Learning Rate')\n",
        "    axes[0].set_ylabel('Success (1) / Failure (0)')\n",
        "    axes[0].set_ylim(-0.1, 1.1)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Muon stability\n",
        "    muon_lrs_all = list(all_results['muon'].keys())\n",
        "    muon_success = [1 if not all_results['muon'][lr]['failed'] else 0 for lr in muon_lrs_all]\n",
        "\n",
        "    axes[1].semilogx(muon_lrs_all, muon_success, 's-', color='red', linewidth=2, markersize=8)\n",
        "    axes[1].set_title('Muon Training Stability')\n",
        "    axes[1].set_xlabel('Learning Rate')\n",
        "    axes[1].set_ylabel('Success (1) / Failure (0)')\n",
        "    axes[1].set_ylim(-0.1, 1.1)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{results_dir}/lr_stability_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def generate_lr_report(all_results: Dict, results_dir: str):\n",
        "    \"\"\"Generate comprehensive learning rate report\"\"\"\n",
        "\n",
        "    report = []\n",
        "    report.append(\"=\" * 80)\n",
        "    report.append(\"COMPREHENSIVE LEARNING RATE SEARCH: MUON vs ADAMW\")\n",
        "    report.append(\"=\" * 80)\n",
        "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    config = all_results['config']\n",
        "    report.append(\"EXPERIMENTAL SETUP\")\n",
        "    report.append(\"-\" * 40)\n",
        "    report.append(f\"Model: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "    report.append(f\"Training steps: {config.max_steps}\")\n",
        "    report.append(f\"Batch size: {config.batch_size}\")\n",
        "    report.append(f\"Weight decay: {config.weight_decay}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    # Analyze AdamW results\n",
        "    report.append(\"ADAMW RESULTS\")\n",
        "    report.append(\"-\" * 40)\n",
        "\n",
        "    adamw_successful = []\n",
        "    adamw_failed = []\n",
        "\n",
        "    for lr, result in all_results['adamw'].items():\n",
        "        if result['failed']:\n",
        "            adamw_failed.append((lr, result['reason']))\n",
        "        else:\n",
        "            adamw_successful.append((lr, result))\n",
        "\n",
        "    report.append(f\"Successful runs: {len(adamw_successful)}/{len(all_results['adamw'])}\")\n",
        "    report.append(f\"Failed runs: {len(adamw_failed)}\")\n",
        "\n",
        "    if adamw_failed:\n",
        "        report.append(\"Failed learning rates:\")\n",
        "        for lr, reason in adamw_failed:\n",
        "            report.append(f\"  LR {lr}: {reason}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    if adamw_successful:\n",
        "        # Find best learning rate\n",
        "        best_lr, best_result = max(adamw_successful, key=lambda x: x[1]['final_metrics']['val_accuracy'])\n",
        "        report.append(f\"BEST ADAMW LR: {best_lr}\")\n",
        "        report.append(f\"  Final Training Loss: {best_result['final_train_loss']:.4f}\")\n",
        "        report.append(f\"  Validation Loss: {best_result['final_metrics']['val_loss']:.4f}\")\n",
        "        report.append(f\"  Validation Accuracy: {best_result['final_metrics']['val_accuracy']:.4f}\")\n",
        "        report.append(f\"  Validation Perplexity: {best_result['final_metrics']['val_perplexity']:.2f}\")\n",
        "        report.append(f\"  Training Time: {best_result['training_time']:.1f}s\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Learning rate range analysis\n",
        "        lrs = [x[0] for x in adamw_successful]\n",
        "        val_accs = [x[1]['final_metrics']['val_accuracy'] for x in adamw_successful]\n",
        "        report.append(f\"LR Range Analysis:\")\n",
        "        report.append(f\"  Working LR range: {min(lrs)} to {max(lrs)}\")\n",
        "        report.append(f\"  Best val accuracy: {max(val_accs):.4f}\")\n",
        "        report.append(f\"  Worst val accuracy: {min(val_accs):.4f}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Analyze Muon results\n",
        "    report.append(\"MUON RESULTS\")\n",
        "    report.append(\"-\" * 40)\n",
        "\n",
        "    muon_successful = []\n",
        "    muon_failed = []\n",
        "\n",
        "    for lr, result in all_results['muon'].items():\n",
        "        if result['failed']:\n",
        "            muon_failed.append((lr, result['reason']))\n",
        "        else:\n",
        "            muon_successful.append((lr, result))\n",
        "\n",
        "    report.append(f\"Successful runs: {len(muon_successful)}/{len(all_results['muon'])}\")\n",
        "    report.append(f\"Failed runs: {len(muon_failed)}\")\n",
        "\n",
        "    if muon_failed:\n",
        "        report.append(\"Failed learning rates:\")\n",
        "        for lr, reason in muon_failed:\n",
        "            report.append(f\"  LR {lr}: {reason}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    if muon_successful:\n",
        "        # Find best learning rate\n",
        "        best_lr, best_result = max(muon_successful, key=lambda x: x[1]['final_metrics']['val_accuracy'])\n",
        "        report.append(f\"BEST MUON LR: {best_lr}\")\n",
        "        report.append(f\"  Final Training Loss: {best_result['final_train_loss']:.4f}\")\n",
        "        report.append(f\"  Validation Loss: {best_result['final_metrics']['val_loss']:.4f}\")\n",
        "        report.append(f\"  Validation Accuracy: {best_result['final_metrics']['val_accuracy']:.4f}\")\n",
        "        report.append(f\"  Validation Perplexity: {best_result['final_metrics']['val_perplexity']:.2f}\")\n",
        "        report.append(f\"  Training Time: {best_result['training_time']:.1f}s\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Learning rate range analysis\n",
        "        lrs = [x[0] for x in muon_successful]\n",
        "        val_accs = [x[1]['final_metrics']['val_accuracy'] for x in muon_successful]\n",
        "        report.append(f\"LR Range Analysis:\")\n",
        "        report.append(f\"  Working LR range: {min(lrs)} to {max(lrs)}\")\n",
        "        report.append(f\"  Best val accuracy: {max(val_accs):.4f}\")\n",
        "        report.append(f\"  Worst val accuracy: {min(val_accs):.4f}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Comparison\n",
        "    if adamw_successful and muon_successful:\n",
        "        report.append(\"DIRECT COMPARISON\")\n",
        "        report.append(\"-\" * 40)\n",
        "\n",
        "        best_adamw = max(adamw_successful, key=lambda x: x[1]['final_metrics']['val_accuracy'])\n",
        "        best_muon = max(muon_successful, key=lambda x: x[1]['final_metrics']['val_accuracy'])\n",
        "\n",
        "        adamw_acc = best_adamw[1]['final_metrics']['val_accuracy']\n",
        "        muon_acc = best_muon[1]['final_metrics']['val_accuracy']\n",
        "\n",
        "        adamw_loss = best_adamw[1]['final_metrics']['val_loss']\n",
        "        muon_loss = best_muon[1]['final_metrics']['val_loss']\n",
        "\n",
        "        report.append(f\"Best validation accuracy:\")\n",
        "        report.append(f\"  AdamW (LR={best_adamw[0]}): {adamw_acc:.4f}\")\n",
        "        report.append(f\"  Muon (LR={best_muon[0]}):  {muon_acc:.4f}\")\n",
        "        report.append(f\"  Winner: {'Muon' if muon_acc > adamw_acc else 'AdamW'} by {abs(muon_acc - adamw_acc):.4f}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        report.append(f\"Best validation loss:\")\n",
        "        report.append(f\"  AdamW (LR={best_adamw[0]}): {adamw_loss:.4f}\")\n",
        "        report.append(f\"  Muon (LR={best_muon[0]}):  {muon_loss:.4f}\")\n",
        "        report.append(f\"  Winner: {'Muon' if muon_loss < adamw_loss else 'AdamW'} by {abs(muon_loss - adamw_loss):.4f}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Stability comparison\n",
        "        adamw_stable_range = max(lrs for lrs, _ in adamw_successful) / min(lrs for lrs, _ in adamw_successful)\n",
        "        muon_stable_range = max(lrs for lrs, _ in muon_successful) / min(lrs for lrs, _ in muon_successful)\n",
        "\n",
        "        report.append(f\"Learning rate stability:\")\n",
        "        report.append(f\"  AdamW stable range: {adamw_stable_range:.1f}x\")\n",
        "        report.append(f\"  Muon stable range: {muon_stable_range:.1f}x\")\n",
        "        report.append(f\"  More stable: {'AdamW' if adamw_stable_range > muon_stable_range else 'Muon'}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Key findings\n",
        "    report.append(\"KEY FINDINGS\")\n",
        "    report.append(\"-\" * 40)\n",
        "\n",
        "    if adamw_successful and muon_successful:\n",
        "        best_adamw_acc = max(x[1]['final_metrics']['val_accuracy'] for x in adamw_successful)\n",
        "        best_muon_acc = max(x[1]['final_metrics']['val_accuracy'] for x in muon_successful)\n",
        "\n",
        "        if best_muon_acc > best_adamw_acc:\n",
        "            report.append(\"✓ Muon achieves higher peak performance than AdamW\")\n",
        "        else:\n",
        "            report.append(\"✗ AdamW achieves higher peak performance than Muon\")\n",
        "\n",
        "        if len(adamw_successful) > len(muon_successful):\n",
        "            report.append(\"✓ AdamW is more stable across learning rates\")\n",
        "        elif len(muon_successful) > len(adamw_successful):\n",
        "            report.append(\"✓ Muon is more stable across learning rates\")\n",
        "        else:\n",
        "            report.append(\"≈ Both optimizers show similar stability\")\n",
        "\n",
        "    # Save report\n",
        "    with open(f'{results_dir}/lr_search_report.txt', 'w') as f:\n",
        "        f.write('\\n'.join(report))\n",
        "\n",
        "def save_lr_raw_data(all_results: Dict, results_dir: str):\n",
        "    \"\"\"Save raw learning rate search data\"\"\"\n",
        "\n",
        "    # Prepare serializable data\n",
        "    raw_data = {\n",
        "        'config': {\n",
        "            'd_model': all_results['config'].d_model,\n",
        "            'n_layers': all_results['config'].n_layers,\n",
        "            'n_heads': all_results['config'].n_heads,\n",
        "            'd_ff': all_results['config'].d_ff,\n",
        "            'max_steps': all_results['config'].max_steps,\n",
        "            'batch_size': all_results['config'].batch_size,\n",
        "        },\n",
        "        'results': {}\n",
        "    }\n",
        "\n",
        "    for optimizer_type in ['adamw', 'muon']:\n",
        "        raw_data['results'][optimizer_type] = {}\n",
        "        for lr, result in all_results[optimizer_type].items():\n",
        "            raw_data['results'][optimizer_type][str(lr)] = {\n",
        "                'failed': result['failed'],\n",
        "                'learning_rate': result['learning_rate'],\n",
        "                'training_time': result.get('training_time', 0),\n",
        "                'final_train_loss': result.get('final_train_loss'),\n",
        "                'final_metrics': result.get('final_metrics', {}),\n",
        "                'failure_reason': result.get('reason') if result['failed'] else None,\n",
        "                'failure_step': result.get('step') if result['failed'] else None\n",
        "            }\n",
        "\n",
        "            # Add metrics if available\n",
        "            if not result['failed'] and 'tracker' in result:\n",
        "                raw_data['results'][optimizer_type][str(lr)]['metrics'] = result['tracker'].metrics\n",
        "\n",
        "    with open(f'{results_dir}/lr_search_raw_data.json', 'w') as f:\n",
        "        json.dump(raw_data, f, indent=2, default=str)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check system\n",
        "    print(f\"🔍 Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Set global seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Run learning rate search\n",
        "    start_time = time.time()\n",
        "    results = run_learning_rate_search()\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n🎉 LEARNING RATE SEARCH COMPLETED IN {total_time/60:.1f} MINUTES!\")\n",
        "    print(\"✅ Results saved to 'results/' folder\")\n",
        "    print(\"📊 Check the generated plots and report for optimal learning rates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nefyRRN6RHCq"
      },
      "source": [
        "📊 Using optimal learning rates from LR search:\n",
        "   - AdamW: 0.003\n",
        "   - Muon:  0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjHyqzo7P7f5"
      },
      "source": [
        "## Model size ablations with best learning rate for both"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqxBbFkmHk7o"
      },
      "source": [
        "Cell below was run on Nvidia GTX 4090, if you are running it on free Google Colab GPU or weaker GPU:\n",
        "- Copy code into AI and ask it to estimate if you will run out of CUDA memory\n",
        "- Tell AI (Claude Sonnet is good) to saver results after each step so if later experiments cause out of memory, your results are saved\n",
        "- Possibly tell AI to reduce size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "26c9a6dd654b414eb111a0b6a8043935",
            "5cde147b327344978d07e64deff87a12",
            "1daae211ad8d444780992a490e904aa1",
            "cddb11c23d6a4c4c94c39f6a1f6bc636",
            "93aee02e442046e1875b4a15a6a30af5",
            "7a55c7c877794cf5bccad42b06cebea8",
            "a2efb11554aa4abdb86f566ba078d7ce",
            "137b94ac0e584330bb4a7d570f4b9cde",
            "7f723b5293a847e0912d8ff19f88fb5d",
            "2818c34877b145f6b8124f1d683d3b96",
            "54c54fcd5b0c4dbbb5be17bbbd1ef966",
            "fc1421621cc449539b521904afafbf2c",
            "d3e1c9d0d1e74bb3a3ffd92e8a762754",
            "be21a0843dde428cbac02419a79e7fcb",
            "3d0426fe116249f1bdbc3cbd25dfb9a8",
            "cd036ac9496946388c34541097830d6f",
            "d3ec095505f349fdbdbcc59879bfc7c1",
            "03801f3b1efe41fcafbc0aa451e78cae",
            "79c1b25d335345eb96a0674ac8afc5c9",
            "420a7ef70b5544209068d1974137774d",
            "204bc2c006704e6d8c91631d4388009c",
            "d68e0c9c057e4ba88a4e3828d874d33c"
          ]
        },
        "id": "8k2yEDHAP_SU",
        "outputId": "0f0b6001-75f0-4723-8781-ecaec653e9a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Device: CUDA\n",
            "GPU: NVIDIA GeForce RTX 4090\n",
            "Memory: 25.3 GB\n",
            "🌱 Set all seeds to 42\n",
            "\n",
            "🚀 Starting comprehensive model size ablation with 2 runs per configuration\n",
            "⏱️ Estimated time: 2-4 hours (depending on hardware)\n",
            "💾 Results will be automatically saved with timestamps\n",
            "🚀 COMPREHENSIVE ABLATION: 2 runs per configuration\n",
            "================================================================================\n",
            "📊 Using optimal learning rates:\n",
            "   AdamW: 0.003\n",
            "   Muon:  0.01\n",
            "🔬 Enhanced with: gradient accumulation, dropout, longer training\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "🔬 TESTING TINY MODEL\n",
            "   Architecture: 192d, 4L, 6H, 768ff\n",
            "   Training: 6000 steps, batch size 32\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "================================================================================\n",
            "📦 Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "✅ Loaded 2000 documents, 500,000 tokens from cache\n",
            "\n",
            "📊 ADAMW Run 1/2\n",
            "\n",
            "🚀 Training ADAMW on Tiny (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 11,208,384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 6000/6000 [04:34<00:00, 21.84it/s, loss=1.7671, acc=0.571, ppl=5.9, lr=2.72e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 274.8 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.8373, Acc: 0.7875, PPL: 2.31\n",
            "\n",
            "📊 ADAMW Run 2/2\n",
            "\n",
            "🚀 Training ADAMW on Tiny (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 11,208,384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 6000/6000 [04:33<00:00, 21.97it/s, loss=1.7058, acc=0.576, ppl=5.5, lr=2.72e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 273.1 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.7851, Acc: 0.8006, PPL: 2.19\n",
            "\n",
            "📊 MUON Run 1/2\n",
            "\n",
            "🚀 Training MUON on Tiny (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 11,208,384\n",
            "  Muon parameters: 1,769,472\n",
            "  AdamW parameters: 9,438,912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 6000/6000 [04:45<00:00, 21.00it/s, loss=1.8745, acc=0.569, ppl=6.5, lr=9.07e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 285.7 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.9444, Acc: 0.7830, PPL: 2.57\n",
            "\n",
            "📊 MUON Run 2/2\n",
            "\n",
            "🚀 Training MUON on Tiny (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 11,208,384\n",
            "  Muon parameters: 1,769,472\n",
            "  AdamW parameters: 9,438,912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 6000/6000 [04:41<00:00, 21.31it/s, loss=1.8783, acc=0.568, ppl=6.5, lr=9.07e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 281.5 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.9423, Acc: 0.7841, PPL: 2.57\n",
            "\n",
            "================================================================================\n",
            "🔬 TESTING SMALL MODEL\n",
            "   Architecture: 384d, 6L, 8H, 1536ff\n",
            "   Training: 5000 steps, batch size 24\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "================================================================================\n",
            "📦 Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "✅ Loaded 2000 documents, 500,000 tokens from cache\n",
            "\n",
            "📊 ADAMW Run 1/2\n",
            "\n",
            "🚀 Training ADAMW on Small (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 29,496,192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 5000/5000 [04:28<00:00, 18.65it/s, loss=0.5567, acc=0.850, ppl=1.7, lr=2.72e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 268.2 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.1948, Acc: 0.9494, PPL: 1.22\n",
            "\n",
            "📊 ADAMW Run 2/2\n",
            "\n",
            "🚀 Training ADAMW on Small (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 29,496,192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 5000/5000 [04:28<00:00, 18.63it/s, loss=0.6559, acc=0.826, ppl=1.9, lr=2.72e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 268.3 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.1930, Acc: 0.9497, PPL: 1.21\n",
            "\n",
            "📊 MUON Run 1/2\n",
            "\n",
            "🚀 Training MUON on Small (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 29,496,192\n",
            "  Muon parameters: 10,616,832\n",
            "  AdamW parameters: 18,879,360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 5000/5000 [04:37<00:00, 17.99it/s, loss=0.5100, acc=0.874, ppl=1.7, lr=9.07e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 277.9 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.1582, Acc: 0.9626, PPL: 1.17\n",
            "\n",
            "📊 MUON Run 2/2\n",
            "\n",
            "🚀 Training MUON on Small (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 29,496,192\n",
            "  Muon parameters: 10,616,832\n",
            "  AdamW parameters: 18,879,360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 5000/5000 [04:38<00:00, 17.98it/s, loss=0.5751, acc=0.858, ppl=1.8, lr=9.07e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 278.1 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.1593, Acc: 0.9623, PPL: 1.17\n",
            "\n",
            "================================================================================\n",
            "🔬 TESTING MEDIUM MODEL\n",
            "   Architecture: 512d, 8L, 8H, 2048ff\n",
            "   Training: 4000 steps, batch size 16\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "================================================================================\n",
            "📦 Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "✅ Loaded 2000 documents, 500,000 tokens from cache\n",
            "\n",
            "📊 ADAMW Run 1/2\n",
            "\n",
            "🚀 Training ADAMW on Medium (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 50,340,352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 4000/4000 [03:23<00:00, 19.68it/s, loss=0.7908, acc=0.790, ppl=2.2, lr=2.72e-03]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 203.3 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.3210, Acc: 0.9143, PPL: 1.38\n",
            "\n",
            "📊 ADAMW Run 2/2\n",
            "\n",
            "🚀 Training ADAMW on Medium (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 50,340,352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 4000/4000 [03:22<00:00, 19.73it/s, loss=0.7070, acc=0.812, ppl=2.0, lr=2.72e-03]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 202.7 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.2986, Acc: 0.9204, PPL: 1.35\n",
            "\n",
            "📊 MUON Run 1/2\n",
            "\n",
            "🚀 Training MUON on Medium (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 50,340,352\n",
            "  Muon parameters: 25,165,824\n",
            "  AdamW parameters: 25,174,528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 4000/4000 [03:31<00:00, 18.91it/s, loss=0.4998, acc=0.882, ppl=1.6, lr=9.08e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 211.5 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.1644, Acc: 0.9604, PPL: 1.18\n",
            "\n",
            "📊 MUON Run 2/2\n",
            "\n",
            "🚀 Training MUON on Medium (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 50,340,352\n",
            "  Muon parameters: 25,165,824\n",
            "  AdamW parameters: 25,174,528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 4000/4000 [03:31<00:00, 18.91it/s, loss=0.4573, acc=0.890, ppl=1.6, lr=9.08e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 211.5 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.1575, Acc: 0.9624, PPL: 1.17\n",
            "\n",
            "================================================================================\n",
            "🔬 TESTING LARGE MODEL\n",
            "   Architecture: 768d, 10L, 16H, 3072ff\n",
            "   Training: 3000 steps, batch size 12\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "================================================================================\n",
            "📦 Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "✅ Loaded 2000 documents, 500,000 tokens from cache\n",
            "\n",
            "📊 ADAMW Run 1/2\n",
            "\n",
            "🚀 Training ADAMW on Large (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 108,543,744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 3000/3000 [03:25<00:00, 14.61it/s, loss=4.1460, acc=0.249, ppl=63.2, lr=2.73e-03]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 205.3 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 3.8141, Acc: 0.2791, PPL: 45.33\n",
            "\n",
            "📊 ADAMW Run 2/2\n",
            "\n",
            "🚀 Training ADAMW on Large (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 108,543,744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ADAMW: 100%|██████████| 3000/3000 [03:25<00:00, 14.62it/s, loss=4.1773, acc=0.247, ppl=65.2, lr=2.73e-03]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 205.2 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 3.7074, Acc: 0.2878, PPL: 40.75\n",
            "\n",
            "📊 MUON Run 1/2\n",
            "\n",
            "🚀 Training MUON on Large (Run 1)\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 108,543,744\n",
            "  Muon parameters: 70,778,880\n",
            "  AdamW parameters: 37,764,864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 3000/3000 [03:35<00:00, 13.91it/s, loss=0.5369, acc=0.858, ppl=1.7, lr=9.09e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 215.7 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.2260, Acc: 0.9453, PPL: 1.25\n",
            "\n",
            "📊 MUON Run 2/2\n",
            "\n",
            "🚀 Training MUON on Large (Run 2)\n",
            "🌱 Set all seeds to 1042\n",
            "  📊 Total parameters: 108,543,744\n",
            "  Muon parameters: 70,778,880\n",
            "  AdamW parameters: 37,764,864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MUON: 100%|██████████| 3000/3000 [03:35<00:00, 13.90it/s, loss=0.4284, acc=0.901, ppl=1.5, lr=9.09e-03]    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ⏱️ Training completed in 215.8 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  📊 Final - Loss: 0.2161, Acc: 0.9475, PPL: 1.24\n",
            "\n",
            "💾 Saving comprehensive results to results/comprehensive_ablation_20250721_205356\n",
            "✅ Comprehensive results saved to results/comprehensive_ablation_20250721_205356\n",
            "\n",
            "🎉 COMPREHENSIVE ABLATION COMPLETED!\n",
            "⏱️ Total time: 1.1 hours\n",
            "📊 Results saved to: results/comprehensive_ablation_20250721_205356\n",
            "✅ Check the generated plots and comprehensive report for detailed analysis\n",
            "🔬 All data cached for future analysis\n",
            "\n",
            "📋 QUICK SUMMARY:\n",
            "  Tiny: AdamW wins (-1.33% accuracy improvement)\n",
            "  Small: Muon wins (+1.36% accuracy improvement)\n",
            "  Medium: Muon wins (+4.80% accuracy improvement)\n",
            "  Large: Muon wins (+233.88% accuracy improvement)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "import subprocess\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"🌱 Set all seeds to {seed}\")\n",
        "\n",
        "@dataclass\n",
        "class ImprovedModelConfig:\n",
        "    # Model architecture (required fields first)\n",
        "    d_model: int\n",
        "    n_heads: int\n",
        "    n_layers: int\n",
        "    d_ff: int\n",
        "    batch_size: int\n",
        "    max_steps: int\n",
        "\n",
        "    # Training parameters - MUCH MORE AGGRESSIVE\n",
        "    gradient_accumulation_steps: int = 4  # Simulate larger batches\n",
        "\n",
        "    # Data parameters - LARGER DATASET\n",
        "    max_seq_len: int = 512  # Longer sequences\n",
        "    num_documents: int = 2000  # 5x more documents\n",
        "    max_tokens: int = 500000  # 8x more tokens\n",
        "\n",
        "    # Evaluation\n",
        "    eval_every: int = 500  # Less frequent but more comprehensive\n",
        "    eval_steps: int = 100  # More validation batches\n",
        "\n",
        "    # Learning rates (from your search)\n",
        "    adamw_lr: float = 0.003\n",
        "    muon_lr: float = 0.01\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay: float = 0.1  # Stronger regularization\n",
        "    dropout: float = 0.1  # Add dropout\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Technical\n",
        "    use_amp: bool = True\n",
        "    compile_model: bool = False\n",
        "    vocab_size: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "def improved_model_configs():\n",
        "    \"\"\"More challenging model configurations\"\"\"\n",
        "    return {\n",
        "        'Tiny': ImprovedModelConfig(\n",
        "            d_model=192, n_heads=6, n_layers=4, d_ff=768,\n",
        "            batch_size=32, max_steps=6000\n",
        "        ),\n",
        "        'Small': ImprovedModelConfig(\n",
        "            d_model=384, n_heads=8, n_layers=6, d_ff=1536,\n",
        "            batch_size=24, max_steps=5000\n",
        "        ),\n",
        "        'Medium': ImprovedModelConfig(\n",
        "            d_model=512, n_heads=8, n_layers=8, d_ff=2048,\n",
        "            batch_size=16, max_steps=4000\n",
        "        ),\n",
        "        'Large': ImprovedModelConfig(\n",
        "            d_model=768, n_heads=16, n_layers=10, d_ff=3072,\n",
        "            batch_size=12, max_steps=3000\n",
        "        )\n",
        "    }\n",
        "\n",
        "@torch.compile\n",
        "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
        "    \"\"\"Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\"\"\"\n",
        "    assert G.ndim >= 2\n",
        "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
        "    X = G.bfloat16()\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.mT\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    return X\n",
        "\n",
        "class Muon(torch.optim.Optimizer):\n",
        "    \"\"\"Muon - MomentUm Orthogonalized by Newton-schulz\"\"\"\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
        "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                g = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                if \"momentum_buffer\" not in state:\n",
        "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
        "\n",
        "                buf = state[\"momentum_buffer\"]\n",
        "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
        "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
        "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
        "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n",
        "\n",
        "def load_and_cache_data(config: ImprovedModelConfig, cache_dir: str = \"data_cache\"):\n",
        "    \"\"\"Load and cache tokenized data to avoid reprocessing\"\"\"\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_file = f\"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl\"\n",
        "\n",
        "    # Check if cached data exists\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"📦 Loading cached data from {cache_file}\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "        texts = cached_data['texts']\n",
        "        tokenizer = cached_data['tokenizer']\n",
        "        tokens = cached_data['tokens']\n",
        "\n",
        "        # Update vocab size in config\n",
        "        config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        print(f\"✅ Loaded {len(texts)} documents, {len(tokens):,} tokens from cache\")\n",
        "        return texts, tokenizer, tokens\n",
        "\n",
        "    print(f\"🔄 Processing new data (will cache for future use)\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", token=False)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True, token=False)\n",
        "\n",
        "    texts = []\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= config.num_documents:\n",
        "            break\n",
        "        texts.append(item[\"text\"][:3000])  # Longer text chunks\n",
        "\n",
        "    print(f\"Loaded {len(texts)} documents\")\n",
        "\n",
        "    # Tokenize\n",
        "    print(\"Tokenizing texts...\")\n",
        "    all_tokens = []\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    # Limit tokens\n",
        "    tokens = all_tokens[:config.max_tokens]\n",
        "    print(f\"Using {len(tokens):,} tokens\")\n",
        "\n",
        "    # Update config\n",
        "    config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Cache the processed data\n",
        "    cached_data = {\n",
        "        'texts': texts,\n",
        "        'tokenizer': tokenizer,\n",
        "        'tokens': tokens\n",
        "    }\n",
        "\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(cached_data, f)\n",
        "\n",
        "    print(f\"💾 Cached data to {cache_file}\")\n",
        "\n",
        "    return texts, tokenizer, tokens\n",
        "\n",
        "class TextTokenDataset(Dataset):\n",
        "    def __init__(self, tokens: List[int], seq_len: int = 512):\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "class Rotary(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        angular_freq = (1 / 10000) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
        "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
        "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
        "        self.register_buffer('cos', theta.cos(), persistent=False)\n",
        "        self.register_buffer('sin', theta.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x_BTHD: torch.Tensor):\n",
        "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
        "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
        "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
        "        y1 = x1 * cos + x2 * sin\n",
        "        y2 = x1 * (-sin) + x2 * cos\n",
        "        return torch.cat((y1, y2), 3).type_as(x_BTHD)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, max_seq_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.rotary = Rotary(self.d_k, max_seq_len)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        Q = self.rotary(Q)\n",
        "        K = self.rotary(K)\n",
        "\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
        "        return self.w_o(attn_output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.silu(self.linear1(x))))\n",
        "\n",
        "class ImprovedTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int,\n",
        "                 max_seq_len: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm with dropout\n",
        "        attn_out = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        ff_out = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n",
        "\n",
        "class ImprovedMinimalLLM(nn.Module):\n",
        "    def __init__(self, config: ImprovedModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            ImprovedTransformerBlock(\n",
        "                config.d_model, config.n_heads, config.d_ff,\n",
        "                config.max_seq_len, config.dropout\n",
        "            ) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.RMSNorm(config.d_model)\n",
        "        self.output_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Tie weights\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embedding.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
        "        x = self.position_dropout(x)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.output_dropout(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "class MetricsTracker:\n",
        "    def __init__(self):\n",
        "        self.metrics = {}\n",
        "        self.memory_usage = []\n",
        "\n",
        "    def log_step(self, step: int, **kwargs):\n",
        "        for key, value in kwargs.items():\n",
        "            if key not in self.metrics:\n",
        "                self.metrics[key] = []\n",
        "            self.metrics[key].append((step, value))\n",
        "\n",
        "    def log_memory(self, step: int):\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            self.memory_usage.append((step, allocated, reserved))\n",
        "\n",
        "def setup_optimizer(model: nn.Module, optimizer_type: str, config: ImprovedModelConfig):\n",
        "    \"\"\"Setup optimizer with optimal learning rates\"\"\"\n",
        "\n",
        "    if optimizer_type == 'muon':\n",
        "        muon_params = []\n",
        "        adamw_params = []\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if (param.ndim == 2 and\n",
        "                'token_embedding' not in name and\n",
        "                'norm' not in name and\n",
        "                param.requires_grad):\n",
        "                muon_params.append(param)\n",
        "            else:\n",
        "                adamw_params.append(param)\n",
        "\n",
        "        print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
        "        print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
        "\n",
        "        muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)\n",
        "        adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)\n",
        "\n",
        "        return [muon_optimizer, adamw_optimizer]\n",
        "\n",
        "    else:  # adamw\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.adamw_lr, weight_decay=config.weight_decay)\n",
        "        return [optimizer]\n",
        "\n",
        "def comprehensive_evaluate_model(model: nn.Module, val_loader: DataLoader,\n",
        "                               config: ImprovedModelConfig) -> Dict:\n",
        "    \"\"\"More comprehensive evaluation with multiple metrics\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "    total_correct_top5 = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            if i >= config.eval_steps:  # More evaluation steps\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            with autocast(enabled=config.use_amp):\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            # Top-1 accuracy\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "            # Top-5 accuracy\n",
        "            top5_predictions = logits.topk(5, dim=-1)[1]\n",
        "            total_correct_top5 += (top5_predictions == y.unsqueeze(-1)).any(dim=-1).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    top5_accuracy = total_correct_top5 / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return {\n",
        "        'val_loss': avg_loss,\n",
        "        'val_accuracy': accuracy,\n",
        "        'val_top5_accuracy': top5_accuracy,\n",
        "        'val_perplexity': perplexity\n",
        "    }\n",
        "\n",
        "def improved_train_model(optimizer_type: str, config: ImprovedModelConfig,\n",
        "                        train_loader: DataLoader, val_loader: DataLoader,\n",
        "                        model_name: str, run_id: int = 0) -> Tuple[MetricsTracker, Dict]:\n",
        "    \"\"\"Improved training with gradient accumulation and better evaluation\"\"\"\n",
        "\n",
        "    print(f\"\\n🚀 Training {optimizer_type.upper()} on {model_name} (Run {run_id+1})\")\n",
        "\n",
        "    # Initialize with different seeds for multiple runs\n",
        "    set_seed(42 + run_id * 1000)\n",
        "    model = ImprovedMinimalLLM(config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    if config.compile_model:\n",
        "        try:\n",
        "            model = torch.compile(model, mode='max-autotune')\n",
        "            print(\"  ✅ Model compiled with max-autotune\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Compilation failed: {e}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  📊 Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Setup optimizers\n",
        "    optimizers = setup_optimizer(model, optimizer_type, config)\n",
        "\n",
        "    # Improved learning rate schedule\n",
        "    schedulers = []\n",
        "    for optimizer in optimizers:\n",
        "        warmup_steps = config.max_steps // 20  # 5% warmup\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                # Cosine annealing to 10% of peak\n",
        "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
        "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "        schedulers.append(scheduler)\n",
        "\n",
        "    scaler = GradScaler() if config.use_amp else None\n",
        "    tracker = MetricsTracker()\n",
        "\n",
        "    # Training state\n",
        "    model.train()\n",
        "    step = 0\n",
        "    accumulated_loss = 0\n",
        "    start_time = time.time()\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience_limit = 2000  # Early stopping patience\n",
        "\n",
        "    pbar = tqdm(total=config.max_steps, desc=f\"{optimizer_type.upper()}\")\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass with gradient accumulation\n",
        "            if config.use_amp:\n",
        "                with autocast():\n",
        "                    logits = model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                    loss = loss / config.gradient_accumulation_steps\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                accumulated_loss += loss.item()\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "                accumulated_loss += loss.item()\n",
        "\n",
        "            # Optimizer step after accumulation\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                if config.use_amp:\n",
        "                    # Unscale and clip gradients\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    # Step optimizers\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.step(optimizer)\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "\n",
        "                # Reset accumulated loss\n",
        "                accumulated_loss = 0\n",
        "\n",
        "            # Logging\n",
        "            if step % 50 == 0:\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    accuracy = (predictions == y).float().mean().item()\n",
        "                    current_loss = loss.item() * config.gradient_accumulation_steps\n",
        "                    perplexity = math.exp(min(current_loss, 20))\n",
        "\n",
        "                tracker.log_step(\n",
        "                    step,\n",
        "                    train_loss=current_loss,\n",
        "                    train_accuracy=accuracy,\n",
        "                    train_perplexity=perplexity,\n",
        "                    grad_norm=grad_norm.item() if 'grad_norm' in locals() else 0,\n",
        "                    learning_rate=optimizers[0].param_groups[0]['lr']\n",
        "                )\n",
        "\n",
        "                if step % 500 == 0:  # Less frequent memory logging\n",
        "                    tracker.log_memory(step)\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{current_loss:.4f}',\n",
        "                    'acc': f'{accuracy:.3f}',\n",
        "                    'ppl': f'{perplexity:.1f}',\n",
        "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}'\n",
        "                })\n",
        "\n",
        "            # Comprehensive evaluation\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = comprehensive_evaluate_model(model, val_loader, config)\n",
        "                for key, value in eval_metrics.items():\n",
        "                    tracker.log_step(step, **{key: value})\n",
        "\n",
        "                # Early stopping check\n",
        "                if eval_metrics['val_loss'] < best_val_loss:\n",
        "                    best_val_loss = eval_metrics['val_loss']\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += config.eval_every\n",
        "\n",
        "                if patience_counter >= patience_limit:\n",
        "                    print(f\"\\n  🛑 Early stopping at step {step} (patience exceeded)\")\n",
        "                    break\n",
        "\n",
        "            step += 1\n",
        "            if step % 50 == 0:  # Update progress bar every 10 steps\n",
        "                pbar.update(50)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"  ⏱️ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    # Final comprehensive evaluation\n",
        "    final_eval = comprehensive_evaluate_model(model, val_loader, config)\n",
        "    print(f\"  📊 Final - Loss: {final_eval['val_loss']:.4f}, \"\n",
        "          f\"Acc: {final_eval['val_accuracy']:.4f}, PPL: {final_eval['val_perplexity']:.2f}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return tracker, {\n",
        "        'training_time': training_time,\n",
        "        'final_metrics': final_eval,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'total_params': total_params,\n",
        "        'steps_completed': step\n",
        "    }\n",
        "\n",
        "def run_comprehensive_ablation(num_runs: int = 3):\n",
        "    \"\"\"Run multiple experiments and average results\"\"\"\n",
        "\n",
        "    print(f\"🚀 COMPREHENSIVE ABLATION: {num_runs} runs per configuration\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"📊 Using optimal learning rates:\")\n",
        "    print(\"   AdamW: 0.003\")\n",
        "    print(\"   Muon:  0.01\")\n",
        "    print(\"🔬 Enhanced with: gradient accumulation, dropout, longer training\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model_configs = improved_model_configs()\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name, config in model_configs.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"🔬 TESTING {model_name.upper()} MODEL\")\n",
        "        print(f\"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "        print(f\"   Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "        print(f\"   Data: {config.max_tokens:,} tokens, seq_len {config.max_seq_len}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Load data once per model\n",
        "        texts, tokenizer, tokens = load_and_cache_data(config)\n",
        "        dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
        "\n",
        "        # Fixed train/val split\n",
        "        val_size = len(dataset) // 10\n",
        "        train_size = len(dataset) - val_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            dataset, [train_size, val_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        model_results = {'adamw': [], 'muon': []}\n",
        "\n",
        "        # Multiple runs per optimizer\n",
        "        for optimizer_type in ['adamw', 'muon']:\n",
        "            for run_id in range(num_runs):\n",
        "                print(f\"\\n📊 {optimizer_type.upper()} Run {run_id+1}/{num_runs}\")\n",
        "\n",
        "                tracker, run_results = improved_train_model(\n",
        "                    optimizer_type, config, train_loader, val_loader,\n",
        "                    model_name, run_id\n",
        "                )\n",
        "\n",
        "                model_results[optimizer_type].append({\n",
        "                    'tracker': tracker,\n",
        "                    **run_results\n",
        "                })\n",
        "\n",
        "        all_results[model_name] = {\n",
        "            'config': config,\n",
        "            'results': model_results\n",
        "        }\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def generate_comprehensive_plots(all_results: Dict, results_dir: str):\n",
        "    \"\"\"Generate comprehensive ablation plots with multiple runs\"\"\"\n",
        "\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "\n",
        "    model_names = list(all_results.keys())\n",
        "\n",
        "    # 1. Training Loss Curves with Error Bars\n",
        "    fig, axes = plt.subplots(2, len(model_names), figsize=(6*len(model_names), 10))\n",
        "    if len(model_names) == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "\n",
        "    fig.suptitle('Training Metrics: Muon vs AdamW (Mean ± Std)', fontsize=16, fontweight='bold')\n",
        "\n",
        "    for idx, (model_name, model_data) in enumerate(all_results.items()):\n",
        "        # Training Loss\n",
        "        ax_loss = axes[0, idx]\n",
        "        ax_acc = axes[1, idx]\n",
        "\n",
        "        for optimizer_type in ['adamw', 'muon']:\n",
        "            runs = model_data['results'][optimizer_type]\n",
        "            color = 'blue' if optimizer_type == 'adamw' else 'red'\n",
        "\n",
        "            # Collect all runs data\n",
        "            all_loss_curves = []\n",
        "            all_acc_curves = []\n",
        "\n",
        "            for run in runs:\n",
        "                tracker = run['tracker']\n",
        "                if 'train_loss' in tracker.metrics:\n",
        "                    steps, losses = zip(*tracker.metrics['train_loss'])\n",
        "                    all_loss_curves.append((steps, losses))\n",
        "                if 'train_accuracy' in tracker.metrics:\n",
        "                    steps, accs = zip(*tracker.metrics['train_accuracy'])\n",
        "                    all_acc_curves.append((steps, accs))\n",
        "\n",
        "            # Plot mean with std bands\n",
        "            if all_loss_curves:\n",
        "                # Find common steps\n",
        "                min_len = min(len(curve[1]) for curve in all_loss_curves)\n",
        "                common_steps = all_loss_curves[0][0][:min_len]\n",
        "                loss_matrix = np.array([curve[1][:min_len] for curve in all_loss_curves])\n",
        "\n",
        "                mean_loss = np.mean(loss_matrix, axis=0)\n",
        "                std_loss = np.std(loss_matrix, axis=0)\n",
        "\n",
        "                ax_loss.plot(common_steps, mean_loss, color=color, linewidth=2, label=f'{optimizer_type.upper()}')\n",
        "                ax_loss.fill_between(common_steps, mean_loss - std_loss, mean_loss + std_loss,\n",
        "                                   color=color, alpha=0.2)\n",
        "\n",
        "            if all_acc_curves:\n",
        "                min_len = min(len(curve[1]) for curve in all_acc_curves)\n",
        "                common_steps = all_acc_curves[0][0][:min_len]\n",
        "                acc_matrix = np.array([curve[1][:min_len] for curve in all_acc_curves])\n",
        "\n",
        "                mean_acc = np.mean(acc_matrix, axis=0)\n",
        "                std_acc = np.std(acc_matrix, axis=0)\n",
        "\n",
        "                ax_acc.plot(common_steps, mean_acc, color=color, linewidth=2, label=f'{optimizer_type.upper()}')\n",
        "                ax_acc.fill_between(common_steps, mean_acc - std_acc, mean_acc + std_acc,\n",
        "                                  color=color, alpha=0.2)\n",
        "\n",
        "        config = model_data['config']\n",
        "        total_params = model_data['results']['adamw'][0]['total_params']\n",
        "\n",
        "        ax_loss.set_title(f'{model_name} Training Loss\\n({total_params:,} params)')\n",
        "        ax_loss.set_xlabel('Steps')\n",
        "        ax_loss.set_ylabel('Training Loss')\n",
        "        ax_loss.set_yscale('log')\n",
        "        ax_loss.legend()\n",
        "        ax_loss.grid(True, alpha=0.3)\n",
        "\n",
        "        ax_acc.set_title(f'{model_name} Training Accuracy')\n",
        "        ax_acc.set_xlabel('Steps')\n",
        "        ax_acc.set_ylabel('Training Accuracy')\n",
        "        ax_acc.legend()\n",
        "        ax_acc.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{results_dir}/training_curves_with_uncertainty.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Final Performance Comparison with Error Bars\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Final Performance: Muon vs AdamW (Mean ± Std)', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Collect aggregated metrics\n",
        "    model_params = []\n",
        "    adamw_metrics = {'val_loss': [], 'val_accuracy': [], 'val_perplexity': [], 'training_time': []}\n",
        "    muon_metrics = {'val_loss': [], 'val_accuracy': [], 'val_perplexity': [], 'training_time': []}\n",
        "    adamw_stds = {'val_loss': [], 'val_accuracy': [], 'val_perplexity': [], 'training_time': []}\n",
        "    muon_stds = {'val_loss': [], 'val_accuracy': [], 'val_perplexity': [], 'training_time': []}\n",
        "\n",
        "    for model_name, model_data in all_results.items():\n",
        "        model_params.append(model_data['results']['adamw'][0]['total_params'])\n",
        "\n",
        "        for optimizer_type in ['adamw', 'muon']:\n",
        "            runs = model_data['results'][optimizer_type]\n",
        "\n",
        "            # Collect metrics from all runs\n",
        "            val_losses = [run['final_metrics']['val_loss'] for run in runs]\n",
        "            val_accs = [run['final_metrics']['val_accuracy'] for run in runs]\n",
        "            val_ppls = [run['final_metrics']['val_perplexity'] for run in runs]\n",
        "            times = [run['training_time'] for run in runs]\n",
        "\n",
        "            metrics_dict = adamw_metrics if optimizer_type == 'adamw' else muon_metrics\n",
        "            stds_dict = adamw_stds if optimizer_type == 'adamw' else muon_stds\n",
        "\n",
        "            metrics_dict['val_loss'].append(np.mean(val_losses))\n",
        "            metrics_dict['val_accuracy'].append(np.mean(val_accs))\n",
        "            metrics_dict['val_perplexity'].append(np.mean(val_ppls))\n",
        "            metrics_dict['training_time'].append(np.mean(times))\n",
        "\n",
        "            stds_dict['val_loss'].append(np.std(val_losses))\n",
        "            stds_dict['val_accuracy'].append(np.std(val_accs))\n",
        "            stds_dict['val_perplexity'].append(np.std(val_ppls))\n",
        "            stds_dict['training_time'].append(np.std(times))\n",
        "\n",
        "    # Plot with error bars\n",
        "    axes[0,0].errorbar(model_params, adamw_metrics['val_loss'], yerr=adamw_stds['val_loss'],\n",
        "                      fmt='o-', label='AdamW', color='blue', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[0,0].errorbar(model_params, muon_metrics['val_loss'], yerr=muon_stds['val_loss'],\n",
        "                      fmt='s-', label='Muon', color='red', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[0,0].set_title('Validation Loss vs Model Size')\n",
        "    axes[0,0].set_xlabel('Parameters')\n",
        "    axes[0,0].set_ylabel('Validation Loss')\n",
        "    axes[0,0].set_xscale('log')\n",
        "    axes[0,0].set_yscale('log')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0,1].errorbar(model_params, adamw_metrics['val_accuracy'], yerr=adamw_stds['val_accuracy'],\n",
        "                      fmt='o-', label='AdamW', color='blue', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[0,1].errorbar(model_params, muon_metrics['val_accuracy'], yerr=muon_stds['val_accuracy'],\n",
        "                      fmt='s-', label='Muon', color='red', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[0,1].set_title('Validation Accuracy vs Model Size')\n",
        "    axes[0,1].set_xlabel('Parameters')\n",
        "    axes[0,1].set_ylabel('Validation Accuracy')\n",
        "    axes[0,1].set_xscale('log')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1,0].errorbar(model_params, adamw_metrics['val_perplexity'], yerr=adamw_stds['val_perplexity'],\n",
        "                      fmt='o-', label='AdamW', color='blue', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[1,0].errorbar(model_params, muon_metrics['val_perplexity'], yerr=muon_stds['val_perplexity'],\n",
        "                      fmt='s-', label='Muon', color='red', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[1,0].set_title('Validation Perplexity vs Model Size')\n",
        "    axes[1,0].set_xlabel('Parameters')\n",
        "    axes[1,0].set_ylabel('Validation Perplexity')\n",
        "    axes[1,0].set_xscale('log')\n",
        "    axes[1,0].set_yscale('log')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1,1].errorbar(model_params, adamw_metrics['training_time'], yerr=adamw_stds['training_time'],\n",
        "                      fmt='o-', label='AdamW', color='blue', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[1,1].errorbar(model_params, muon_metrics['training_time'], yerr=muon_stds['training_time'],\n",
        "                      fmt='s-', label='Muon', color='red', linewidth=2, markersize=8, capsize=5)\n",
        "    axes[1,1].set_title('Training Time vs Model Size')\n",
        "    axes[1,1].set_xlabel('Parameters')\n",
        "    axes[1,1].set_ylabel('Training Time (seconds)')\n",
        "    axes[1,1].set_xscale('log')\n",
        "    axes[1,1].legend()\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{results_dir}/final_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def generate_comprehensive_report(all_results: Dict, results_dir: str, num_runs: int):\n",
        "    \"\"\"Generate comprehensive ablation report with statistical analysis\"\"\"\n",
        "\n",
        "    report = []\n",
        "    report.append(\"=\" * 80)\n",
        "    report.append(\"COMPREHENSIVE MODEL SIZE ABLATION: MUON vs ADAMW\")\n",
        "    report.append(\"=\" * 80)\n",
        "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    report.append(f\"Number of runs per configuration: {num_runs}\")\n",
        "    report.append(\"\")\n",
        "    report.append(\"EXPERIMENTAL SETUP\")\n",
        "    report.append(\"-\" * 40)\n",
        "    report.append(\"Enhanced experimental setup with:\")\n",
        "    report.append(\"  • Optimal learning rates (AdamW: 0.003, Muon: 0.01)\")\n",
        "    report.append(\"  • Gradient accumulation (4 steps)\")\n",
        "    report.append(\"  • Dropout regularization (0.1)\")\n",
        "    report.append(\"  • Longer sequences (512 tokens)\")\n",
        "    report.append(\"  • Larger dataset (500k tokens, 2000 documents)\")\n",
        "    report.append(\"  • Extended training (6k-12k steps)\")\n",
        "    report.append(\"  • Multiple runs for statistical significance\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    # Model configurations\n",
        "    report.append(\"Model Configurations:\")\n",
        "    for model_name, model_data in all_results.items():\n",
        "        config = model_data['config']\n",
        "        total_params = model_data['results']['adamw'][0]['total_params']\n",
        "        report.append(f\"  {model_name}: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "        report.append(f\"    Parameters: {total_params:,}\")\n",
        "        report.append(f\"    Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    # Detailed results for each model\n",
        "    all_improvements = {'acc': [], 'loss': [], 'ppl': [], 'time': []}\n",
        "\n",
        "    for model_name, model_data in all_results.items():\n",
        "        report.append(f\"{'='*60}\")\n",
        "        report.append(f\"{model_name.upper()} MODEL RESULTS\")\n",
        "        report.append(f\"{'='*60}\")\n",
        "\n",
        "        config = model_data['config']\n",
        "        total_params = model_data['results']['adamw'][0]['total_params']\n",
        "\n",
        "        report.append(f\"Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "        report.append(f\"Parameters: {total_params:,}\")\n",
        "        report.append(f\"Training steps: {config.max_steps}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Aggregate metrics across runs\n",
        "        adamw_runs = model_data['results']['adamw']\n",
        "        muon_runs = model_data['results']['muon']\n",
        "\n",
        "        # Calculate means and stds\n",
        "        adamw_metrics = {\n",
        "            'val_loss': [run['final_metrics']['val_loss'] for run in adamw_runs],\n",
        "            'val_accuracy': [run['final_metrics']['val_accuracy'] for run in adamw_runs],\n",
        "            'val_perplexity': [run['final_metrics']['val_perplexity'] for run in adamw_runs],\n",
        "            'training_time': [run['training_time'] for run in adamw_runs]\n",
        "        }\n",
        "\n",
        "        muon_metrics = {\n",
        "            'val_loss': [run['final_metrics']['val_loss'] for run in muon_runs],\n",
        "            'val_accuracy': [run['final_metrics']['val_accuracy'] for run in muon_runs],\n",
        "            'val_perplexity': [run['final_metrics']['val_perplexity'] for run in muon_runs],\n",
        "            'training_time': [run['training_time'] for run in muon_runs]\n",
        "        }\n",
        "\n",
        "        report.append(\"FINAL PERFORMANCE METRICS (Mean ± Std)\")\n",
        "        report.append(\"-\" * 45)\n",
        "        report.append(f\"                    AdamW              Muon               Δ (p-value)\")\n",
        "\n",
        "        # Statistical tests and reporting\n",
        "        for metric_key, metric_name in [('val_loss', 'Val Loss'), ('val_accuracy', 'Val Accuracy'),\n",
        "                                       ('val_perplexity', 'Val Perplexity'), ('training_time', 'Training Time')]:\n",
        "            adamw_values = adamw_metrics[metric_key]\n",
        "            muon_values = muon_metrics[metric_key]\n",
        "\n",
        "            adamw_mean, adamw_std = np.mean(adamw_values), np.std(adamw_values)\n",
        "            muon_mean, muon_std = np.mean(muon_values), np.std(muon_values)\n",
        "\n",
        "            # Statistical test\n",
        "            if len(adamw_values) > 1 and len(muon_values) > 1:\n",
        "                t_stat, p_value = stats.ttest_ind(muon_values, adamw_values)\n",
        "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
        "            else:\n",
        "                p_value = 1.0\n",
        "                significance = \"\"\n",
        "\n",
        "            # Improvement calculation\n",
        "            if metric_key in ['val_loss', 'val_perplexity', 'training_time']:\n",
        "                improvement = (adamw_mean - muon_mean) / adamw_mean * 100  # Lower is better\n",
        "            else:\n",
        "                improvement = (muon_mean - adamw_mean) / adamw_mean * 100  # Higher is better\n",
        "\n",
        "            report.append(f\"{metric_name:15s}: {adamw_mean:6.4f}±{adamw_std:5.4f}   {muon_mean:6.4f}±{muon_std:5.4f}   {improvement:+6.1f}% (p={p_value:.3f}){significance}\")\n",
        "\n",
        "            # Store for overall analysis\n",
        "            if metric_key == 'val_accuracy':\n",
        "                all_improvements['acc'].append(improvement)\n",
        "            elif metric_key == 'val_loss':\n",
        "                all_improvements['loss'].append(improvement)\n",
        "            elif metric_key == 'val_perplexity':\n",
        "                all_improvements['ppl'].append(improvement)\n",
        "            elif metric_key == 'training_time':\n",
        "                all_improvements['time'].append(improvement)\n",
        "\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Winner determination\n",
        "        muon_acc_mean = np.mean(muon_metrics['val_accuracy'])\n",
        "        adamw_acc_mean = np.mean(adamw_metrics['val_accuracy'])\n",
        "\n",
        "        if muon_acc_mean > adamw_acc_mean:\n",
        "            report.append(\"🏆 WINNER: Muon (higher validation accuracy)\")\n",
        "        else:\n",
        "            report.append(\"🏆 WINNER: AdamW (higher validation accuracy)\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Overall analysis\n",
        "    report.append(\"=\" * 80)\n",
        "    report.append(\"OVERALL ANALYSIS\")\n",
        "    report.append(\"=\" * 80)\n",
        "\n",
        "    # Count wins\n",
        "    muon_wins = sum(1 for imp in all_improvements['acc'] if imp > 0)\n",
        "    total_models = len(all_improvements['acc'])\n",
        "\n",
        "    report.append(\"MUON vs ADAMW PERFORMANCE\")\n",
        "    report.append(\"-\" * 35)\n",
        "    report.append(f\"Validation Accuracy Wins: Muon {muon_wins}/{total_models}, AdamW {total_models-muon_wins}/{total_models}\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    report.append(\"AVERAGE IMPROVEMENTS (Muon vs AdamW)\")\n",
        "    report.append(\"-\" * 40)\n",
        "\n",
        "    for metric, values in all_improvements.items():\n",
        "        metric_name = {'acc': 'Validation Accuracy', 'loss': 'Validation Loss',\n",
        "                      'ppl': 'Validation Perplexity', 'time': 'Training Time'}[metric]\n",
        "\n",
        "        mean_imp = np.mean(values)\n",
        "        std_imp = np.std(values)\n",
        "\n",
        "        # Statistical significance test\n",
        "        if len(values) > 1:\n",
        "            t_stat, p_value = stats.ttest_1samp(values, 0)\n",
        "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
        "        else:\n",
        "            p_value = 1.0\n",
        "            significance = \"\"\n",
        "\n",
        "        report.append(f\"{metric_name:20s}: {mean_imp:+6.2f}% ± {std_imp:5.2f}% (p={p_value:.3f}){significance}\")\n",
        "\n",
        "    report.append(\"\")\n",
        "\n",
        "    # Key findings\n",
        "    report.append(\"KEY FINDINGS\")\n",
        "    report.append(\"-\" * 20)\n",
        "\n",
        "    acc_improvement = np.mean(all_improvements['acc'])\n",
        "    loss_improvement = np.mean(all_improvements['loss'])\n",
        "    time_improvement = np.mean(all_improvements['time'])\n",
        "\n",
        "    if acc_improvement > 0:\n",
        "        report.append(\"✓ Muon consistently outperforms AdamW in validation accuracy\")\n",
        "    else:\n",
        "        report.append(\"✗ AdamW outperforms Muon in validation accuracy\")\n",
        "\n",
        "    if loss_improvement > 0:\n",
        "        report.append(\"✓ Muon achieves lower validation loss than AdamW\")\n",
        "    else:\n",
        "        report.append(\"✗ AdamW achieves lower validation loss than Muon\")\n",
        "\n",
        "    if abs(time_improvement) < 5:\n",
        "        report.append(\"≈ Similar training times between optimizers\")\n",
        "    elif time_improvement > 0:\n",
        "        report.append(\"⚡ Muon is faster than AdamW\")\n",
        "    else:\n",
        "        report.append(\"⚠ Muon is slower than AdamW\")\n",
        "\n",
        "    # Statistical significance summary\n",
        "    report.append(\"\")\n",
        "    report.append(\"STATISTICAL SIGNIFICANCE\")\n",
        "    report.append(\"-\" * 30)\n",
        "\n",
        "    significant_metrics = []\n",
        "    for metric, values in all_improvements.items():\n",
        "        if len(values) > 1:\n",
        "            _, p_value = stats.ttest_1samp(values, 0)\n",
        "            if p_value < 0.05:\n",
        "                metric_name = {'acc': 'Accuracy', 'loss': 'Loss', 'ppl': 'Perplexity', 'time': 'Time'}[metric]\n",
        "                significant_metrics.append(metric_name)\n",
        "\n",
        "    if significant_metrics:\n",
        "        report.append(f\"Significant improvements (p < 0.05): {', '.join(significant_metrics)}\")\n",
        "    else:\n",
        "        report.append(\"No statistically significant differences found\")\n",
        "\n",
        "    # Scaling behavior\n",
        "    if len(all_improvements['acc']) >= 2:\n",
        "        report.append(\"\")\n",
        "        report.append(\"SCALING BEHAVIOR\")\n",
        "        report.append(\"-\" * 20)\n",
        "\n",
        "        # Check correlation with model size\n",
        "        model_params = [all_results[name]['results']['adamw'][0]['total_params'] for name in all_results.keys()]\n",
        "        correlation, p_value = stats.pearsonr(model_params, all_improvements['acc'])\n",
        "\n",
        "        if correlation > 0.5 and p_value < 0.05:\n",
        "            report.append(\"📈 Muon's advantage significantly increases with model size\")\n",
        "        elif correlation < -0.5 and p_value < 0.05:\n",
        "            report.append(\"📉 Muon's advantage significantly decreases with model size\")\n",
        "        else:\n",
        "            report.append(\"📊 No clear scaling trend observed\")\n",
        "\n",
        "    # Save report\n",
        "    with open(f'{results_dir}/comprehensive_ablation_report.txt', 'w') as f:\n",
        "        f.write('\\n'.join(report))\n",
        "\n",
        "def save_comprehensive_results(all_results: Dict, results_dir: str, num_runs: int):\n",
        "    \"\"\"Save all results and generate comprehensive analysis\"\"\"\n",
        "\n",
        "    # Create results directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    full_results_dir = f\"{results_dir}/comprehensive_ablation_{timestamp}\"\n",
        "    os.makedirs(full_results_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n💾 Saving comprehensive results to {full_results_dir}\")\n",
        "\n",
        "    # Generate plots\n",
        "    generate_comprehensive_plots(all_results, full_results_dir)\n",
        "\n",
        "    # Generate report\n",
        "    generate_comprehensive_report(all_results, full_results_dir, num_runs)\n",
        "\n",
        "    # Save raw data\n",
        "    raw_data = {}\n",
        "    for model_name, model_data in all_results.items():\n",
        "        raw_data[model_name] = {\n",
        "            'config': {\n",
        "                'd_model': model_data['config'].d_model,\n",
        "                'n_layers': model_data['config'].n_layers,\n",
        "                'n_heads': model_data['config'].n_heads,\n",
        "                'd_ff': model_data['config'].d_ff,\n",
        "                'max_steps': model_data['config'].max_steps,\n",
        "                'batch_size': model_data['config'].batch_size,\n",
        "                'gradient_accumulation_steps': model_data['config'].gradient_accumulation_steps,\n",
        "                'dropout': model_data['config'].dropout,\n",
        "                'max_seq_len': model_data['config'].max_seq_len,\n",
        "                'max_tokens': model_data['config'].max_tokens,\n",
        "            },\n",
        "            'results': {}\n",
        "        }\n",
        "\n",
        "        for optimizer_type in ['adamw', 'muon']:\n",
        "            runs_data = []\n",
        "            for run in model_data['results'][optimizer_type]:\n",
        "                runs_data.append({\n",
        "                    'training_time': run['training_time'],\n",
        "                    'final_metrics': run['final_metrics'],\n",
        "                    'best_val_loss': run['best_val_loss'],\n",
        "                    'total_params': run['total_params'],\n",
        "                    'steps_completed': run['steps_completed'],\n",
        "                    'metrics': run['tracker'].metrics,\n",
        "                    'memory_usage': run['tracker'].memory_usage\n",
        "                })\n",
        "            raw_data[model_name]['results'][optimizer_type] = runs_data\n",
        "\n",
        "    with open(f'{full_results_dir}/comprehensive_raw_data.json', 'w') as f:\n",
        "        json.dump(raw_data, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"✅ Comprehensive results saved to {full_results_dir}\")\n",
        "    return full_results_dir\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check system\n",
        "    print(f\"🔍 Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Set global seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Configuration\n",
        "    NUM_RUNS = 2  # Number of runs per configuration\n",
        "\n",
        "    print(f\"\\n🚀 Starting comprehensive model size ablation with {NUM_RUNS} runs per configuration\")\n",
        "    print(\"⏱️ Estimated time: 2-4 hours (depending on hardware)\")\n",
        "    print(\"💾 Results will be automatically saved with timestamps\")\n",
        "\n",
        "    # Run comprehensive ablation\n",
        "    start_time = time.time()\n",
        "    results = run_comprehensive_ablation(num_runs=NUM_RUNS)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Save results\n",
        "    results_dir = save_comprehensive_results(results, \"results\", NUM_RUNS)\n",
        "\n",
        "    print(f\"\\n🎉 COMPREHENSIVE ABLATION COMPLETED!\")\n",
        "    print(f\"⏱️ Total time: {total_time/3600:.1f} hours\")\n",
        "    print(f\"📊 Results saved to: {results_dir}\")\n",
        "    print(\"✅ Check the generated plots and comprehensive report for detailed analysis\")\n",
        "    print(\"🔬 All data cached for future analysis\")\n",
        "\n",
        "    # Quick summary\n",
        "    print(f\"\\n📋 QUICK SUMMARY:\")\n",
        "    for model_name, model_data in results.items():\n",
        "        adamw_acc = np.mean([run['final_metrics']['val_accuracy'] for run in model_data['results']['adamw']])\n",
        "        muon_acc = np.mean([run['final_metrics']['val_accuracy'] for run in model_data['results']['muon']])\n",
        "        improvement = (muon_acc - adamw_acc) / adamw_acc * 100\n",
        "        winner = \"Muon\" if muon_acc > adamw_acc else \"AdamW\"\n",
        "        print(f\"  {model_name}: {winner} wins ({improvement:+.2f}% accuracy improvement)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This code just fixes a bug when uploading this notebook to github, you can ignore it otherwise."
      ],
      "metadata": {
        "id": "3wx-6rjiH0Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from ipywidgets import Widget\n",
        "\n",
        "# Close all widgets and clear their state\n",
        "Widget.close_all()\n"
      ],
      "metadata": {
        "id": "5FME3Pk_HpCs"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}