# ğŸ“ˆ Muon vs AdamW: Optimizer Scaling Dynamics in Transformer Training



This repository contains the full LaTeX source for our empirical study:

**Scaling Dynamics of Muon versus AdamW: An Empirical Analysis of Optimizer Performance in Transformer Language Models**

> **Authors**: Vuk RosiÄ‡ (Ã“buda University), Claude (Anthropic)  
> **Date**: July 21, 2025

## ğŸ§  Overview

Optimizer choice critically impacts large language model (LLM) training. While **AdamW** remains the default in transformer-based architectures, **Muon**â€”a newer optimizer incorporating **gradient orthogonalization via Newton-Schulz iteration**â€”shows promise for improved convergence at scale.

This study compares **AdamW** and **Muon** across four model sizes (11M to 108M parameters) trained on the **SmolLM-Corpus**. Results show:

- ğŸŸ¦ **AdamW** outperforms on small models  
- ğŸ”´ **Muon** scales better, outperforming AdamW dramatically on large models  
- ğŸ’¥ **AdamW catastrophically fails** at 108M parameters  
- âš™ï¸ **Muon incurs only 4â€“5% compute overhead**

## ğŸ“„ Paper Highlights

- ğŸ“Š **Learning Rate Sweeps** show Muon is more stable
- ğŸš€ **Scaling Analysis** reveals divergence at 108M scale
- ğŸ§® **Gradient Orthogonalization** improves training stability
- ğŸ’¡ **Practical Implications** for choosing optimizers in LLM training

## ğŸ“‚ Contents

```bash
.
â”œâ”€â”€ results/                      # Plots and figures used in the paper
â”‚   â”œâ”€â”€ experiment_1_learning_rate/
â”‚   â””â”€â”€ experiment_2_model_size/
â”œâ”€â”€ main.tex                     # Full LaTeX source of the paper
â”œâ”€â”€ references.bib               # Bibliography entries (if split)
â”œâ”€â”€ README.md                    # You're here!
````

## ğŸ“¸ Key Figures

* ğŸ” **Learning Rate Sensitivity**: AdamW peaks sharply, Muon is broader
* ğŸ“ˆ **Training Curves**: AdamW flattens early at large scale; Muon improves steadily
* ğŸ§ª **Validation Accuracy**: Muon reaches **94.6%** on 108M model vs **28.4%** for AdamW
* â±ï¸ **Compute Overhead**: \~4.2% for Muon

## ğŸ“š Citation

If you use or refer to this work, please cite:

```bibtex
@article{rosic2025muon,
  title={Scaling Dynamics of Muon versus AdamW: An Empirical Analysis of Optimizer Performance in Transformer Language Models},
  author={RosiÄ‡, Vuk and Claude},
  journal={arXiv preprint},
  year={2025}
}
```

## ğŸ§ª Reproducibility Notes

* Dataset: SmolLM-Corpus (500k tokens)
* Hardware: T4 GPU (limited scale)
* Framework: PyTorch + custom optimizer
* Training length varies by model size to equalize compute

## ğŸ“¬ Contact

For questions or collaborations, feel free to reach out:

* Vuk RosiÄ‡ â€“ [vukrosic1@gmail.com](mailto:vukrosic1@gmail.com)